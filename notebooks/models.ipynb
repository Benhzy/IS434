{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS434 Twitter/X Bot Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Project aims to use machine learning and analytics techniques to detect bot accounts on the Twitter/X platform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgmpy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (0.1.26)\n",
      "Requirement already satisfied: networkx in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (2.2.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (3.1.4)\n",
      "Requirement already satisfied: torch in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (2.4.1)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (0.14.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.4.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (3.4.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (2.1.1)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.20.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.148.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.35.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (5.28.2)\n",
      "Requirement already satisfied: pydantic in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pandas->pgmpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pandas->pgmpy) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pandas->pgmpy) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn->pgmpy) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from statsmodels->pgmpy) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from statsmodels->pgmpy) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tqdm->pgmpy) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core->google-generativeai->pgmpy) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core->google-generativeai->pgmpy) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (4.9)\n",
      "Requirement already satisfied: six in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from patsy>=0.5.6->statsmodels->pgmpy) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from jinja2->torch->pgmpy) (3.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pydantic->google-generativeai->pgmpy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pydantic->google-generativeai->pgmpy) (2.23.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from sympy->torch->pgmpy) (1.3.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.66.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.66.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->pgmpy) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2024.8.30)\n",
      "Requirement already satisfied: gensim in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: imblearn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Requirement already satisfied: keras in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (0.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.17.0-cp312-cp312-win_amd64.whl (2.0 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
      "   ---------------------------------------- 0.0/385.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/385.2 MB 6.3 MB/s eta 0:01:02\n",
      "   ---------------------------------------- 2.4/385.2 MB 5.6 MB/s eta 0:01:09\n",
      "   ---------------------------------------- 2.9/385.2 MB 6.0 MB/s eta 0:01:04\n",
      "   ---------------------------------------- 3.9/385.2 MB 4.7 MB/s eta 0:01:22\n",
      "    --------------------------------------- 5.5/385.2 MB 5.3 MB/s eta 0:01:12\n",
      "    --------------------------------------- 7.3/385.2 MB 5.9 MB/s eta 0:01:05\n",
      "    --------------------------------------- 8.4/385.2 MB 5.7 MB/s eta 0:01:07\n",
      "    --------------------------------------- 9.4/385.2 MB 5.6 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 10.7/385.2 MB 5.6 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 12.1/385.2 MB 5.7 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 13.1/385.2 MB 5.7 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 14.4/385.2 MB 5.7 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 15.5/385.2 MB 5.7 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 16.8/385.2 MB 5.7 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 18.4/385.2 MB 5.8 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 20.2/385.2 MB 6.0 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 21.8/385.2 MB 6.1 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 23.1/385.2 MB 6.2 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 24.4/385.2 MB 6.1 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 25.7/385.2 MB 6.1 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 27.0/385.2 MB 6.1 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 28.0/385.2 MB 6.1 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 29.4/385.2 MB 6.1 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 30.7/385.2 MB 6.1 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 32.0/385.2 MB 6.1 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 33.0/385.2 MB 6.1 MB/s eta 0:00:58\n",
      "   --- ------------------------------------ 34.3/385.2 MB 6.1 MB/s eta 0:00:58\n",
      "   --- ------------------------------------ 35.7/385.2 MB 6.1 MB/s eta 0:00:58\n",
      "   --- ------------------------------------ 37.0/385.2 MB 6.1 MB/s eta 0:00:58\n",
      "   --- ------------------------------------ 38.3/385.2 MB 6.1 MB/s eta 0:00:58\n",
      "   ---- ----------------------------------- 39.6/385.2 MB 6.1 MB/s eta 0:00:57\n",
      "   ---- ----------------------------------- 40.9/385.2 MB 6.1 MB/s eta 0:00:57\n",
      "   ---- ----------------------------------- 42.5/385.2 MB 6.1 MB/s eta 0:00:57\n",
      "   ---- ----------------------------------- 43.0/385.2 MB 6.0 MB/s eta 0:00:58\n",
      "   ---- ----------------------------------- 44.0/385.2 MB 6.0 MB/s eta 0:00:58\n",
      "   ---- ----------------------------------- 45.4/385.2 MB 6.0 MB/s eta 0:00:57\n",
      "   ---- ----------------------------------- 46.7/385.2 MB 6.0 MB/s eta 0:00:57\n",
      "   ---- ----------------------------------- 48.0/385.2 MB 6.0 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 49.3/385.2 MB 6.0 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 50.6/385.2 MB 6.0 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 51.9/385.2 MB 6.0 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 53.2/385.2 MB 6.0 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 54.5/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 56.1/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 56.9/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 58.2/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 59.2/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 60.3/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 61.6/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 62.9/385.2 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 64.2/385.2 MB 6.0 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 65.5/385.2 MB 6.0 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 67.1/385.2 MB 6.0 MB/s eta 0:00:54\n",
      "   ------- -------------------------------- 67.9/385.2 MB 6.0 MB/s eta 0:00:54\n",
      "   ------- -------------------------------- 69.5/385.2 MB 6.0 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 70.3/385.2 MB 6.0 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 71.6/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 72.6/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 73.7/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 75.0/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 76.0/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 77.1/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 78.4/385.2 MB 5.9 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 79.7/385.2 MB 5.9 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 81.0/385.2 MB 5.9 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 82.1/385.2 MB 5.9 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 83.4/385.2 MB 5.9 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 84.7/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   -------- ------------------------------- 85.7/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 87.0/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 88.3/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 89.1/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 90.4/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 91.5/385.2 MB 5.9 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 92.8/385.2 MB 5.9 MB/s eta 0:00:50\n",
      "   --------- ------------------------------ 94.1/385.2 MB 5.9 MB/s eta 0:00:50\n",
      "   --------- ------------------------------ 95.4/385.2 MB 5.9 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 96.7/385.2 MB 5.9 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 98.0/385.2 MB 5.9 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 98.6/385.2 MB 5.9 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 99.9/385.2 MB 5.8 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 101.4/385.2 MB 5.9 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 103.3/385.2 MB 5.9 MB/s eta 0:00:48\n",
      "   ---------- ----------------------------- 104.6/385.2 MB 5.9 MB/s eta 0:00:48\n",
      "   ---------- ----------------------------- 105.6/385.2 MB 5.9 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 107.0/385.2 MB 5.9 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 108.5/385.2 MB 5.9 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 109.8/385.2 MB 5.9 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 110.9/385.2 MB 5.9 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 112.2/385.2 MB 5.9 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 113.5/385.2 MB 5.9 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 115.1/385.2 MB 5.9 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 116.1/385.2 MB 5.9 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 117.4/385.2 MB 5.9 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 118.8/385.2 MB 5.9 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 120.1/385.2 MB 5.9 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 121.4/385.2 MB 5.9 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 122.7/385.2 MB 5.9 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 124.0/385.2 MB 5.9 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 125.3/385.2 MB 5.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 126.4/385.2 MB 5.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 127.4/385.2 MB 5.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 128.7/385.2 MB 5.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 130.3/385.2 MB 5.9 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 131.3/385.2 MB 5.9 MB/s eta 0:00:43\n",
      "   ------------- -------------------------- 132.6/385.2 MB 5.9 MB/s eta 0:00:43\n",
      "   ------------- -------------------------- 133.7/385.2 MB 5.9 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 135.0/385.2 MB 5.9 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 136.3/385.2 MB 5.9 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 137.4/385.2 MB 5.9 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 138.7/385.2 MB 5.9 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 139.7/385.2 MB 5.9 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 141.0/385.2 MB 5.9 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 142.6/385.2 MB 5.9 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 144.2/385.2 MB 5.9 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 145.5/385.2 MB 5.9 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 146.8/385.2 MB 5.9 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 147.8/385.2 MB 5.9 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 149.4/385.2 MB 5.9 MB/s eta 0:00:40\n",
      "   --------------- ------------------------ 150.7/385.2 MB 5.9 MB/s eta 0:00:40\n",
      "   --------------- ------------------------ 152.0/385.2 MB 5.9 MB/s eta 0:00:40\n",
      "   --------------- ------------------------ 153.1/385.2 MB 5.9 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 154.7/385.2 MB 5.9 MB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 156.5/385.2 MB 6.0 MB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 158.3/385.2 MB 6.0 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 159.9/385.2 MB 6.0 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 161.0/385.2 MB 6.0 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 162.3/385.2 MB 6.0 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 163.6/385.2 MB 6.0 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 164.9/385.2 MB 6.0 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 165.9/385.2 MB 6.0 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 167.2/385.2 MB 6.0 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 168.3/385.2 MB 6.0 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 169.6/385.2 MB 6.0 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 170.7/385.2 MB 6.0 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 171.7/385.2 MB 6.0 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 173.3/385.2 MB 6.0 MB/s eta 0:00:36\n",
      "   ------------------ --------------------- 175.4/385.2 MB 6.0 MB/s eta 0:00:35\n",
      "   ------------------ --------------------- 176.9/385.2 MB 6.0 MB/s eta 0:00:35\n",
      "   ------------------ --------------------- 178.8/385.2 MB 6.0 MB/s eta 0:00:35\n",
      "   ------------------ --------------------- 180.1/385.2 MB 6.0 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 181.7/385.2 MB 6.1 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 181.9/385.2 MB 6.0 MB/s eta 0:00:34\n",
      "   ------------------- -------------------- 183.0/385.2 MB 6.0 MB/s eta 0:00:34\n",
      "   ------------------- -------------------- 184.3/385.2 MB 6.1 MB/s eta 0:00:34\n",
      "   ------------------- -------------------- 185.3/385.2 MB 6.1 MB/s eta 0:00:34\n",
      "   ------------------- -------------------- 186.6/385.2 MB 6.0 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 187.7/385.2 MB 6.0 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 189.0/385.2 MB 6.0 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 190.3/385.2 MB 6.0 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 191.9/385.2 MB 6.0 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 193.5/385.2 MB 6.1 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 195.0/385.2 MB 6.1 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 196.3/385.2 MB 6.1 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 197.4/385.2 MB 6.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 198.7/385.2 MB 6.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 199.8/385.2 MB 6.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 201.1/385.2 MB 6.0 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 202.4/385.2 MB 6.0 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 203.7/385.2 MB 6.0 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 204.7/385.2 MB 6.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 205.8/385.2 MB 6.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 207.1/385.2 MB 6.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 208.1/385.2 MB 6.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 209.7/385.2 MB 6.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 211.3/385.2 MB 6.0 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 213.1/385.2 MB 6.0 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 214.2/385.2 MB 6.0 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 215.5/385.2 MB 6.0 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 217.1/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 217.8/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 218.6/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 219.4/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 219.7/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 220.7/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 221.2/385.2 MB 5.9 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 222.8/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 223.9/385.2 MB 6.0 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 225.2/385.2 MB 6.0 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 226.5/385.2 MB 6.0 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 227.8/385.2 MB 5.9 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 229.1/385.2 MB 6.0 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 230.2/385.2 MB 5.9 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 231.5/385.2 MB 5.9 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 233.3/385.2 MB 6.0 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 234.4/385.2 MB 6.0 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 235.4/385.2 MB 6.0 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 236.7/385.2 MB 6.0 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 238.0/385.2 MB 6.0 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 239.1/385.2 MB 6.0 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 240.4/385.2 MB 6.0 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 241.7/385.2 MB 6.0 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 243.0/385.2 MB 6.0 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 244.3/385.2 MB 6.0 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 246.2/385.2 MB 6.0 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 248.0/385.2 MB 6.0 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 249.3/385.2 MB 6.0 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 250.6/385.2 MB 6.0 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 251.1/385.2 MB 6.0 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 252.4/385.2 MB 6.0 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 253.0/385.2 MB 6.0 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 254.5/385.2 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 255.9/385.2 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 257.4/385.2 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 258.2/385.2 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 259.0/385.2 MB 6.0 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 260.3/385.2 MB 6.0 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 261.6/385.2 MB 6.0 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 262.7/385.2 MB 6.0 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 264.0/385.2 MB 6.0 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 265.3/385.2 MB 6.0 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 266.6/385.2 MB 6.0 MB/s eta 0:00:20\n",
      "   --------------------------- ------------ 267.9/385.2 MB 6.0 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 270.0/385.2 MB 6.0 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 271.6/385.2 MB 6.0 MB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 272.9/385.2 MB 6.0 MB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 274.2/385.2 MB 6.0 MB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 275.3/385.2 MB 6.0 MB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 276.6/385.2 MB 6.0 MB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 277.9/385.2 MB 6.0 MB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 278.9/385.2 MB 6.0 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 280.2/385.2 MB 6.1 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 281.5/385.2 MB 6.1 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 282.6/385.2 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 283.9/385.2 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 285.2/385.2 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 286.8/385.2 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 288.4/385.2 MB 6.1 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 289.7/385.2 MB 6.0 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 290.7/385.2 MB 6.0 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 292.0/385.2 MB 6.0 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 293.3/385.2 MB 6.0 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 294.6/385.2 MB 6.0 MB/s eta 0:00:15\n",
      "   ------------------------------ --------- 296.0/385.2 MB 6.0 MB/s eta 0:00:15\n",
      "   ------------------------------ --------- 297.0/385.2 MB 6.0 MB/s eta 0:00:15\n",
      "   ------------------------------ --------- 298.1/385.2 MB 6.0 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 299.6/385.2 MB 6.0 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 301.5/385.2 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 302.8/385.2 MB 6.1 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 304.1/385.2 MB 6.1 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 305.4/385.2 MB 6.1 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 306.4/385.2 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 307.8/385.2 MB 6.1 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 308.8/385.2 MB 6.1 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 310.1/385.2 MB 6.0 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 311.2/385.2 MB 6.0 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 312.5/385.2 MB 6.0 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 313.8/385.2 MB 6.0 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 314.8/385.2 MB 6.0 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 316.1/385.2 MB 6.0 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 317.5/385.2 MB 6.0 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 318.8/385.2 MB 6.0 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 320.3/385.2 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 321.7/385.2 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 323.0/385.2 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 324.3/385.2 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 325.6/385.2 MB 6.0 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 326.9/385.2 MB 6.0 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 328.2/385.2 MB 6.0 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 329.3/385.2 MB 6.0 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 330.6/385.2 MB 6.0 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 331.9/385.2 MB 6.0 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 333.2/385.2 MB 6.0 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 334.5/385.2 MB 6.0 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 335.8/385.2 MB 6.0 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 337.1/385.2 MB 6.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 338.4/385.2 MB 6.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 339.7/385.2 MB 6.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 341.0/385.2 MB 6.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 341.8/385.2 MB 6.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 343.1/385.2 MB 6.0 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 344.5/385.2 MB 6.0 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 345.5/385.2 MB 6.0 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 346.8/385.2 MB 6.0 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 348.4/385.2 MB 6.0 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 349.7/385.2 MB 6.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 350.7/385.2 MB 6.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 352.1/385.2 MB 6.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 353.4/385.2 MB 6.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 354.2/385.2 MB 6.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 355.2/385.2 MB 5.9 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 356.0/385.2 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 357.8/385.2 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 358.9/385.2 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 360.2/385.2 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 361.2/385.2 MB 5.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 362.5/385.2 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 363.9/385.2 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 365.2/385.2 MB 5.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 366.5/385.2 MB 5.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 367.8/385.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 369.1/385.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 370.4/385.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 371.5/385.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 372.8/385.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 374.1/385.2 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 375.1/385.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  376.4/385.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  377.7/385.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  378.8/385.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  380.1/385.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  381.2/385.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  382.5/385.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  383.5/385.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  384.8/385.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  385.1/385.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 385.2/385.2 MB 5.8 MB/s eta 0:00:00\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.4/5.5 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Downloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, ml-dtypes, markdown, google-pasta, gast, tensorboard, astunparse, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.2\n",
      "    Uninstalling protobuf-5.28.2:\n",
      "      Successfully uninstalled protobuf-5.28.2\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 protobuf-4.25.5 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-intel-2.17.0 termcolor-2.5.0 werkzeug-3.0.4 wheel-0.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.66.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pgmpy\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install imblearn\n",
    "!pip install keras\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_02c8c_row0_col0, #T_02c8c_row0_col1, #T_02c8c_row0_col2, #T_02c8c_row0_col3, #T_02c8c_row0_col4, #T_02c8c_row0_col5, #T_02c8c_row1_col0, #T_02c8c_row1_col1, #T_02c8c_row1_col2, #T_02c8c_row1_col3, #T_02c8c_row1_col4, #T_02c8c_row1_col5, #T_02c8c_row2_col0, #T_02c8c_row2_col1, #T_02c8c_row2_col2, #T_02c8c_row2_col3, #T_02c8c_row2_col4, #T_02c8c_row2_col5, #T_02c8c_row3_col0, #T_02c8c_row3_col1, #T_02c8c_row3_col2, #T_02c8c_row3_col3, #T_02c8c_row3_col4, #T_02c8c_row3_col5, #T_02c8c_row4_col0, #T_02c8c_row4_col1, #T_02c8c_row4_col2, #T_02c8c_row4_col3, #T_02c8c_row4_col4, #T_02c8c_row4_col5, #T_02c8c_row5_col0, #T_02c8c_row5_col1, #T_02c8c_row5_col2, #T_02c8c_row5_col3, #T_02c8c_row5_col4, #T_02c8c_row5_col5, #T_02c8c_row6_col0, #T_02c8c_row6_col1, #T_02c8c_row6_col2, #T_02c8c_row6_col3, #T_02c8c_row6_col4, #T_02c8c_row6_col5, #T_02c8c_row7_col0, #T_02c8c_row7_col1, #T_02c8c_row7_col2, #T_02c8c_row7_col3, #T_02c8c_row7_col4, #T_02c8c_row7_col5, #T_02c8c_row8_col0, #T_02c8c_row8_col1, #T_02c8c_row8_col2, #T_02c8c_row8_col3, #T_02c8c_row8_col4, #T_02c8c_row8_col5, #T_02c8c_row9_col0, #T_02c8c_row9_col1, #T_02c8c_row9_col2, #T_02c8c_row9_col3, #T_02c8c_row9_col4, #T_02c8c_row9_col5, #T_02c8c_row10_col0, #T_02c8c_row10_col1, #T_02c8c_row10_col2, #T_02c8c_row10_col3, #T_02c8c_row10_col4, #T_02c8c_row10_col5, #T_02c8c_row11_col0, #T_02c8c_row11_col1, #T_02c8c_row11_col2, #T_02c8c_row11_col3, #T_02c8c_row11_col4, #T_02c8c_row11_col5, #T_02c8c_row12_col0, #T_02c8c_row12_col1, #T_02c8c_row12_col2, #T_02c8c_row12_col3, #T_02c8c_row12_col4, #T_02c8c_row12_col5, #T_02c8c_row13_col0, #T_02c8c_row13_col1, #T_02c8c_row13_col2, #T_02c8c_row13_col3, #T_02c8c_row13_col4, #T_02c8c_row13_col5, #T_02c8c_row14_col0, #T_02c8c_row14_col1, #T_02c8c_row14_col2, #T_02c8c_row14_col3, #T_02c8c_row14_col4, #T_02c8c_row14_col5, #T_02c8c_row15_col0, #T_02c8c_row15_col1, #T_02c8c_row15_col2, #T_02c8c_row15_col3, #T_02c8c_row15_col4, #T_02c8c_row15_col5 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_02c8c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_02c8c_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_02c8c_level0_col1\" class=\"col_heading level0 col1\" >hate_speech</th>\n",
       "      <th id=\"T_02c8c_level0_col2\" class=\"col_heading level0 col2\" >offensive_language</th>\n",
       "      <th id=\"T_02c8c_level0_col3\" class=\"col_heading level0 col3\" >neither</th>\n",
       "      <th id=\"T_02c8c_level0_col4\" class=\"col_heading level0 col4\" >class</th>\n",
       "      <th id=\"T_02c8c_level0_col5\" class=\"col_heading level0 col5\" >tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row0\" class=\"row_heading level0 row0\" >67</th>\n",
       "      <td id=\"T_02c8c_row0_col0\" class=\"data row0 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_02c8c_row0_col3\" class=\"data row0 col3\" >2</td>\n",
       "      <td id=\"T_02c8c_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "      <td id=\"T_02c8c_row0_col5\" class=\"data row0 col5\" >\"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these dishes.\" One oreo? Lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row1\" class=\"row_heading level0 row1\" >68</th>\n",
       "      <td id=\"T_02c8c_row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row1_col5\" class=\"data row1 col5\" >\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\" maybe you'll get better. Just http://t.co/TPreVwfq0S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row2\" class=\"row_heading level0 row2\" >69</th>\n",
       "      <td id=\"T_02c8c_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "      <td id=\"T_02c8c_row2_col2\" class=\"data row2 col2\" >2</td>\n",
       "      <td id=\"T_02c8c_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row2_col5\" class=\"data row2 col5\" >\"@Almightywayne__: Fuck Red Malone man bitch ass niggah\" could you please use complete sentences?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row3\" class=\"row_heading level0 row3\" >70</th>\n",
       "      <td id=\"T_02c8c_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_02c8c_row3_col3\" class=\"data row3 col3\" >3</td>\n",
       "      <td id=\"T_02c8c_row3_col4\" class=\"data row3 col4\" >2</td>\n",
       "      <td id=\"T_02c8c_row3_col5\" class=\"data row3 col5\" >\"@ArizonasFinest6: Why the eggplant emoji doe?\"y he say she looked like scream lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row4\" class=\"row_heading level0 row4\" >71</th>\n",
       "      <td id=\"T_02c8c_row4_col0\" class=\"data row4 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row4_col2\" class=\"data row4 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row4_col5\" class=\"data row4 col5\" >\"@AutoWorId: Hennessey Venom GT &#128584; http://t.co/i8eGMnKaJ9\" that's one sexy bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row5\" class=\"row_heading level0 row5\" >72</th>\n",
       "      <td id=\"T_02c8c_row5_col0\" class=\"data row5 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row5_col2\" class=\"data row5 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row5_col4\" class=\"data row5 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row5_col5\" class=\"data row5 col5\" >\"@BOSSBYTCHH: Him seh me pussy wetter then a shower curtain....#ahmesehwetness\"&lt;lmao!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row6\" class=\"row_heading level0 row6\" >73</th>\n",
       "      <td id=\"T_02c8c_row6_col0\" class=\"data row6 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row6_col2\" class=\"data row6 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row6_col4\" class=\"data row6 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row6_col5\" class=\"data row6 col5\" >\"@BRO_HEN314: #Eaglesnation and every #Eagles need to see that pic I just posted because that bitch just said the most racist shit\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row7\" class=\"row_heading level0 row7\" >74</th>\n",
       "      <td id=\"T_02c8c_row7_col0\" class=\"data row7 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "      <td id=\"T_02c8c_row7_col2\" class=\"data row7 col2\" >2</td>\n",
       "      <td id=\"T_02c8c_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row7_col4\" class=\"data row7 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row7_col5\" class=\"data row7 col5\" >\"@BVSEDCHINK: Yo fuck skateboarding, all y'all some wood pushing faggots man, ball is life http://t.co/VBCzP6HMT7\"\n",
       "@AmJemieBenn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row8\" class=\"row_heading level0 row8\" >75</th>\n",
       "      <td id=\"T_02c8c_row8_col0\" class=\"data row8 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row8_col2\" class=\"data row8 col2\" >1</td>\n",
       "      <td id=\"T_02c8c_row8_col3\" class=\"data row8 col3\" >2</td>\n",
       "      <td id=\"T_02c8c_row8_col4\" class=\"data row8 col4\" >2</td>\n",
       "      <td id=\"T_02c8c_row8_col5\" class=\"data row8 col5\" >\"@BabyAnimalPics: baby monkey bathtime http://t.co/7KPWAdLF0R\"\n",
       "Awwwwe! This is soooo ADORABLE!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row9\" class=\"row_heading level0 row9\" >76</th>\n",
       "      <td id=\"T_02c8c_row9_col0\" class=\"data row9 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row9_col2\" class=\"data row9 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row9_col4\" class=\"data row9 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row9_col5\" class=\"data row9 col5\" >\"@BaylaaGottaBody: &#128514;&#128514;&#128514;&#128514; I ain't shit .\" Damn Skippy lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row10\" class=\"row_heading level0 row10\" >77</th>\n",
       "      <td id=\"T_02c8c_row10_col0\" class=\"data row10 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row10_col1\" class=\"data row10 col1\" >1</td>\n",
       "      <td id=\"T_02c8c_row10_col2\" class=\"data row10 col2\" >2</td>\n",
       "      <td id=\"T_02c8c_row10_col3\" class=\"data row10 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row10_col4\" class=\"data row10 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row10_col5\" class=\"data row10 col5\" >\"@BeEasyJrizzy: u ever kill a ant on the sidewalk and think damn what if that nigga was on his way to get some pussy\"No bs must b &gt;30%chance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row11\" class=\"row_heading level0 row11\" >78</th>\n",
       "      <td id=\"T_02c8c_row11_col0\" class=\"data row11 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row11_col2\" class=\"data row11 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row11_col3\" class=\"data row11 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row11_col4\" class=\"data row11 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row11_col5\" class=\"data row11 col5\" >\"@BeenBasedB: @_KudaBrazyy http://t.co/LuUBGL9Y5u\" 0 rings 0 mvps 0 bitches lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row12\" class=\"row_heading level0 row12\" >79</th>\n",
       "      <td id=\"T_02c8c_row12_col0\" class=\"data row12 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row12_col1\" class=\"data row12 col1\" >1</td>\n",
       "      <td id=\"T_02c8c_row12_col2\" class=\"data row12 col2\" >2</td>\n",
       "      <td id=\"T_02c8c_row12_col3\" class=\"data row12 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row12_col4\" class=\"data row12 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row12_col5\" class=\"data row12 col5\" >\"@BeenFLYnSolo: ppl talk bad about the ghetto/hood ... but as a kid growing up, a nigga had funnnnnnn !\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row13\" class=\"row_heading level0 row13\" >80</th>\n",
       "      <td id=\"T_02c8c_row13_col0\" class=\"data row13 col0\" >9</td>\n",
       "      <td id=\"T_02c8c_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row13_col2\" class=\"data row13 col2\" >7</td>\n",
       "      <td id=\"T_02c8c_row13_col3\" class=\"data row13 col3\" >2</td>\n",
       "      <td id=\"T_02c8c_row13_col4\" class=\"data row13 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row13_col5\" class=\"data row13 col5\" >\"@BestProAdvice: The facts on tattoos...tattoo http://t.co/ZwnbhpDZ8e\" he's a pussy with not tattooing them nipples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row14\" class=\"row_heading level0 row14\" >81</th>\n",
       "      <td id=\"T_02c8c_row14_col0\" class=\"data row14 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row14_col1\" class=\"data row14 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row14_col2\" class=\"data row14 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row14_col3\" class=\"data row14 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row14_col4\" class=\"data row14 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row14_col5\" class=\"data row14 col5\" >\"@Bill215_: &#128175;&#128175;&#128175; RT @nel_ayden Bitches be wanting to act like niggas so bad &#128553;&#128553;&#128553; that shit aint cuteeeee\" but niggas act like bitches..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02c8c_level0_row15\" class=\"row_heading level0 row15\" >82</th>\n",
       "      <td id=\"T_02c8c_row15_col0\" class=\"data row15 col0\" >3</td>\n",
       "      <td id=\"T_02c8c_row15_col1\" class=\"data row15 col1\" >0</td>\n",
       "      <td id=\"T_02c8c_row15_col2\" class=\"data row15 col2\" >3</td>\n",
       "      <td id=\"T_02c8c_row15_col3\" class=\"data row15 col3\" >0</td>\n",
       "      <td id=\"T_02c8c_row15_col4\" class=\"data row15 col4\" >1</td>\n",
       "      <td id=\"T_02c8c_row15_col5\" class=\"data row15 col5\" >\"@BitchJones92: Get worshiping bitch! http://t.co/R37CejCjou\" woof woof</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21b6a704560>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('data/labeled_data.csv')\n",
    "# drop unnamed\n",
    "df_train = df_train.drop(columns=['Unnamed: 0'])\n",
    "# show 10 entires from row 47 onwards, and make the tweet column wider, align left\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df_train[67:83].style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benhz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('data/main.csv')\n",
    "df_finetune = pd.read_csv('data/finetune.csv')\n",
    "\n",
    "# Function to clean the tweets (basic preprocessing)\n",
    "def preprocess_text(text):\n",
    "    text = str(text)  \n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))  # Define stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Preprocess each tweet in the dataset\n",
    "df_train['cleaned_tweet'] = df_train['tweet'].apply(preprocess_text)\n",
    "df_finetune['cleaned_tweet'] = df_finetune['tweet'].apply(preprocess_text)\n",
    "\n",
    "# concatenate the two datasets\n",
    "df_combined = pd.concat([df_train, df_finetune])\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=df_combined['cleaned_tweet'], vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save the model for future use\n",
    "model.save(\"word2vec_tweet_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set your maximum sequence length (e.g., 30 words per tweet)\n",
    "max_seq_length = 30\n",
    "embedding_dim = 100  # Same as the vector_size of Word2Vec\n",
    "\n",
    "# get word embeddings for tweet\n",
    "def get_tweet_embeddings(tweet, word2vec_model, embedding_dim):\n",
    "    embeddings = []\n",
    "    for word in tweet:\n",
    "        if word in word2vec_model.wv:\n",
    "            embeddings.append(word2vec_model.wv[word])  # Get the word vector for each word\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))  # Handle out-of-vocabulary words\n",
    "    return embeddings\n",
    "\n",
    "# Convert the entire dataset into sequences of word embeddings\n",
    "tweet_embeddings = [get_tweet_embeddings(tweet, model, embedding_dim) for tweet in df_train['cleaned_tweet']]\n",
    "\n",
    "# Pad the sequences to ensure they are all of the same length (max_seq_length)\n",
    "padded_tweet_embeddings = pad_sequences(tweet_embeddings, maxlen=max_seq_length, dtype='float32', padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_single_vector(tweet, model):\n",
    "    word_vectors = [model.wv[word] for word in tweet if word in model.wv]\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0) \n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tweet_vectors_single'] = df_train['cleaned_tweet'].apply(lambda x: tweet_to_single_vector(x, model))\n",
    "# df_test['tweet_vectors'] = df_test['cleaned_tweet'].apply(lambda x: tweet_to_vector(x, model))\n",
    "\n",
    "# Convert the column of vectors into a numpy array (for model training)\n",
    "tweet_vectors = np.array(df_train['tweet_vectors'].to_list())\n",
    "# tweet_vectors_test = np.array(df_test['tweet_vectors'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tweet_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[user, father, dysfunctional, selfish, drags, ...</td>\n",
       "      <td>[-0.2000133, 0.4018536, 0.02162266, 0.276889, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[user, user, thanks, lyft, credit, cant, use, ...</td>\n",
       "      <td>[-0.07638466, 0.23242302, 0.05301269, 0.322066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[-0.3395153, 0.44796824, 0.26759112, 0.1310546...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, love, u, take, u, time, ur]</td>\n",
       "      <td>[-0.5439526, 1.0469629, -0.45181212, 1.1627967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[-0.2341751, 0.30308622, -0.067414165, -0.1112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "      <td>[ate, user, isz, youuu]</td>\n",
       "      <td>[-0.13588251, 0.23949353, 0.12024341, 0.275756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>[see, nina, turner, airwaves, trying, wrap, ma...</td>\n",
       "      <td>[-0.10440235, 0.16917214, 0.0024858634, 0.1836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>[listening, sad, songs, monday, morning, otw, ...</td>\n",
       "      <td>[-0.21998382, 0.52339184, 0.11752925, 0.245401...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>[user, sikh, temple, vandalised, calgary, wso,...</td>\n",
       "      <td>[-0.23926866, 0.3213547, -0.081885934, -0.0648...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>[thank, user, follow]</td>\n",
       "      <td>[-0.36275133, 0.56216216, 0.21316572, 0.806466...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1      0   @user when a father is dysfunctional and is s...   \n",
       "1          2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2          3      0                                bihday your majesty   \n",
       "3          4      0  #model   i love u take with u all the time in ...   \n",
       "4          5      0             factsguide: society now    #motivation   \n",
       "...      ...    ...                                                ...   \n",
       "31957  31958      0  ate @user isz that youuu?ðððððð...   \n",
       "31958  31959      0    to see nina turner on the airwaves trying to...   \n",
       "31959  31960      0  listening to sad songs on a monday morning otw...   \n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...   \n",
       "31961  31962      0                   thank you @user for you follow     \n",
       "\n",
       "                                           cleaned_tweet  \\\n",
       "0      [user, father, dysfunctional, selfish, drags, ...   \n",
       "1      [user, user, thanks, lyft, credit, cant, use, ...   \n",
       "2                                      [bihday, majesty]   \n",
       "3                    [model, love, u, take, u, time, ur]   \n",
       "4                      [factsguide, society, motivation]   \n",
       "...                                                  ...   \n",
       "31957                            [ate, user, isz, youuu]   \n",
       "31958  [see, nina, turner, airwaves, trying, wrap, ma...   \n",
       "31959  [listening, sad, songs, monday, morning, otw, ...   \n",
       "31960  [user, sikh, temple, vandalised, calgary, wso,...   \n",
       "31961                              [thank, user, follow]   \n",
       "\n",
       "                                           tweet_vectors  \n",
       "0      [-0.2000133, 0.4018536, 0.02162266, 0.276889, ...  \n",
       "1      [-0.07638466, 0.23242302, 0.05301269, 0.322066...  \n",
       "2      [-0.3395153, 0.44796824, 0.26759112, 0.1310546...  \n",
       "3      [-0.5439526, 1.0469629, -0.45181212, 1.1627967...  \n",
       "4      [-0.2341751, 0.30308622, -0.067414165, -0.1112...  \n",
       "...                                                  ...  \n",
       "31957  [-0.13588251, 0.23949353, 0.12024341, 0.275756...  \n",
       "31958  [-0.10440235, 0.16917214, 0.0024858634, 0.1836...  \n",
       "31959  [-0.21998382, 0.52339184, 0.11752925, 0.245401...  \n",
       "31960  [-0.23926866, 0.3213547, -0.081885934, -0.0648...  \n",
       "31961  [-0.36275133, 0.56216216, 0.21316572, 0.806466...  \n",
       "\n",
       "[31962 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    29720\n",
      "1     2242\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_counts = df_train['label'].value_counts()\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tweet_vectors\n",
    "X_rnn = padded_tweet_embeddings\n",
    "y = df_train['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-sampling train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_resampled: (47566, 100)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "print(f'Shape of X_train_resampled: {X_train_resampled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (3197, 100)\n",
      "Shape of X_val: (3196, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_rnn: (25569, 30, 100)\n",
      "Shape of X_test_rnn: (3197, 30, 100)\n",
      "Shape of X_val_rnn: (3196, 30, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X_rnn, y, test_size=0.2, random_state=42)\n",
    "X_val_rnn, X_test_rnn, y_val_rnn, y_test_rnn = train_test_split(X_test_rnn, y_test_rnn, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Shape of X_train_rnn: {X_train_rnn.shape}')\n",
    "print(f'Shape of X_test_rnn: {X_test_rnn.shape}')\n",
    "print(f'Shape of X_val_rnn: {X_val_rnn.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "  epochs = 20\n",
    "  lr = 0.001\n",
    "  use_cuda=False\n",
    "  gamma = 0.7\n",
    "  log_interval = 10\n",
    "  seed = 1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_rnn: torch.Size([25569, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "# load data onto torch\n",
    "X_train_rnn = torch.tensor(X_train_rnn).float()\n",
    "y_train_rnn = torch.tensor(y_train_rnn.values).long()\n",
    "X_val_rnn = torch.tensor(X_val_rnn).float()\n",
    "y_val_rnn = torch.tensor(y_val_rnn.values).long()\n",
    "X_test_rnn = torch.tensor(X_test_rnn).float()\n",
    "y_test_rnn = torch.tensor(y_test_rnn.values).long()\n",
    "\n",
    "print(f'Shape of X_train_rnn: {X_train_rnn.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=100, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(128 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/25569 (0%)]\tLoss: 1.119942\n",
      "Train Epoch: 1 [640/25569 (2%)]\tLoss: 0.310309\n",
      "Train Epoch: 1 [1280/25569 (5%)]\tLoss: 0.330944\n",
      "Train Epoch: 1 [1920/25569 (8%)]\tLoss: 0.103862\n",
      "Train Epoch: 1 [2560/25569 (10%)]\tLoss: 0.453529\n",
      "Train Epoch: 1 [3200/25569 (12%)]\tLoss: 0.267973\n",
      "Train Epoch: 1 [3840/25569 (15%)]\tLoss: 0.279113\n",
      "Train Epoch: 1 [4480/25569 (18%)]\tLoss: 0.275416\n",
      "Train Epoch: 1 [5120/25569 (20%)]\tLoss: 0.267811\n",
      "Train Epoch: 1 [5760/25569 (22%)]\tLoss: 0.112255\n",
      "Train Epoch: 1 [6400/25569 (25%)]\tLoss: 0.149453\n",
      "Train Epoch: 1 [7040/25569 (28%)]\tLoss: 0.353479\n",
      "Train Epoch: 1 [7680/25569 (30%)]\tLoss: 0.282026\n",
      "Train Epoch: 1 [8320/25569 (32%)]\tLoss: 0.571090\n",
      "Train Epoch: 1 [8960/25569 (35%)]\tLoss: 0.173916\n",
      "Train Epoch: 1 [9600/25569 (38%)]\tLoss: 0.237981\n",
      "Train Epoch: 1 [10240/25569 (40%)]\tLoss: 0.200826\n",
      "Train Epoch: 1 [10880/25569 (42%)]\tLoss: 0.162034\n",
      "Train Epoch: 1 [11520/25569 (45%)]\tLoss: 0.195918\n",
      "Train Epoch: 1 [12160/25569 (48%)]\tLoss: 0.155664\n",
      "Train Epoch: 1 [12800/25569 (50%)]\tLoss: 0.186046\n",
      "Train Epoch: 1 [13440/25569 (52%)]\tLoss: 0.273664\n",
      "Train Epoch: 1 [14080/25569 (55%)]\tLoss: 0.351351\n",
      "Train Epoch: 1 [14720/25569 (58%)]\tLoss: 0.282708\n",
      "Train Epoch: 1 [15360/25569 (60%)]\tLoss: 0.382766\n",
      "Train Epoch: 1 [16000/25569 (62%)]\tLoss: 0.296796\n",
      "Train Epoch: 1 [16640/25569 (65%)]\tLoss: 0.329351\n",
      "Train Epoch: 1 [17280/25569 (68%)]\tLoss: 0.097187\n",
      "Train Epoch: 1 [17920/25569 (70%)]\tLoss: 0.321685\n",
      "Train Epoch: 1 [18560/25569 (72%)]\tLoss: 0.214273\n",
      "Train Epoch: 1 [19200/25569 (75%)]\tLoss: 0.325096\n",
      "Train Epoch: 1 [19840/25569 (78%)]\tLoss: 0.313297\n",
      "Train Epoch: 1 [20480/25569 (80%)]\tLoss: 0.251427\n",
      "Train Epoch: 1 [21120/25569 (82%)]\tLoss: 0.286469\n",
      "Train Epoch: 1 [21760/25569 (85%)]\tLoss: 0.162112\n",
      "Train Epoch: 1 [22400/25569 (88%)]\tLoss: 0.233788\n",
      "Train Epoch: 1 [23040/25569 (90%)]\tLoss: 0.281077\n",
      "Train Epoch: 1 [23680/25569 (92%)]\tLoss: 0.429406\n",
      "Train Epoch: 1 [24320/25569 (95%)]\tLoss: 0.336518\n",
      "Train Epoch: 1 [24960/25569 (98%)]\tLoss: 0.316809\n",
      "\n",
      "Test set: Average loss: 0.2679, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/25569 (0%)]\tLoss: 0.248756\n",
      "Train Epoch: 2 [640/25569 (2%)]\tLoss: 0.211716\n",
      "Train Epoch: 2 [1280/25569 (5%)]\tLoss: 0.416239\n",
      "Train Epoch: 2 [1920/25569 (8%)]\tLoss: 0.198144\n",
      "Train Epoch: 2 [2560/25569 (10%)]\tLoss: 0.163928\n",
      "Train Epoch: 2 [3200/25569 (12%)]\tLoss: 0.319011\n",
      "Train Epoch: 2 [3840/25569 (15%)]\tLoss: 0.232928\n",
      "Train Epoch: 2 [4480/25569 (18%)]\tLoss: 0.154553\n",
      "Train Epoch: 2 [5120/25569 (20%)]\tLoss: 0.233937\n",
      "Train Epoch: 2 [5760/25569 (22%)]\tLoss: 0.319413\n",
      "Train Epoch: 2 [6400/25569 (25%)]\tLoss: 0.233252\n",
      "Train Epoch: 2 [7040/25569 (28%)]\tLoss: 0.328550\n",
      "Train Epoch: 2 [7680/25569 (30%)]\tLoss: 0.237509\n",
      "Train Epoch: 2 [8320/25569 (32%)]\tLoss: 0.165318\n",
      "Train Epoch: 2 [8960/25569 (35%)]\tLoss: 0.452395\n",
      "Train Epoch: 2 [9600/25569 (38%)]\tLoss: 0.376641\n",
      "Train Epoch: 2 [10240/25569 (40%)]\tLoss: 0.320934\n",
      "Train Epoch: 2 [10880/25569 (42%)]\tLoss: 0.335940\n",
      "Train Epoch: 2 [11520/25569 (45%)]\tLoss: 0.273308\n",
      "Train Epoch: 2 [12160/25569 (48%)]\tLoss: 0.465823\n",
      "Train Epoch: 2 [12800/25569 (50%)]\tLoss: 0.245623\n",
      "Train Epoch: 2 [13440/25569 (52%)]\tLoss: 0.164717\n",
      "Train Epoch: 2 [14080/25569 (55%)]\tLoss: 0.165914\n",
      "Train Epoch: 2 [14720/25569 (58%)]\tLoss: 0.298874\n",
      "Train Epoch: 2 [15360/25569 (60%)]\tLoss: 0.240672\n",
      "Train Epoch: 2 [16000/25569 (62%)]\tLoss: 0.245685\n",
      "Train Epoch: 2 [16640/25569 (65%)]\tLoss: 0.262149\n",
      "Train Epoch: 2 [17280/25569 (68%)]\tLoss: 0.390469\n",
      "Train Epoch: 2 [17920/25569 (70%)]\tLoss: 0.353548\n",
      "Train Epoch: 2 [18560/25569 (72%)]\tLoss: 0.213353\n",
      "Train Epoch: 2 [19200/25569 (75%)]\tLoss: 0.382897\n",
      "Train Epoch: 2 [19840/25569 (78%)]\tLoss: 0.266762\n",
      "Train Epoch: 2 [20480/25569 (80%)]\tLoss: 0.280840\n",
      "Train Epoch: 2 [21120/25569 (82%)]\tLoss: 0.120722\n",
      "Train Epoch: 2 [21760/25569 (85%)]\tLoss: 0.197357\n",
      "Train Epoch: 2 [22400/25569 (88%)]\tLoss: 0.112475\n",
      "Train Epoch: 2 [23040/25569 (90%)]\tLoss: 0.332084\n",
      "Train Epoch: 2 [23680/25569 (92%)]\tLoss: 0.237791\n",
      "Train Epoch: 2 [24320/25569 (95%)]\tLoss: 0.271087\n",
      "Train Epoch: 2 [24960/25569 (98%)]\tLoss: 0.361748\n",
      "\n",
      "Test set: Average loss: 0.2589, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/25569 (0%)]\tLoss: 0.195449\n",
      "Train Epoch: 3 [640/25569 (2%)]\tLoss: 0.227401\n",
      "Train Epoch: 3 [1280/25569 (5%)]\tLoss: 0.366587\n",
      "Train Epoch: 3 [1920/25569 (8%)]\tLoss: 0.146386\n",
      "Train Epoch: 3 [2560/25569 (10%)]\tLoss: 0.401424\n",
      "Train Epoch: 3 [3200/25569 (12%)]\tLoss: 0.280669\n",
      "Train Epoch: 3 [3840/25569 (15%)]\tLoss: 0.191781\n",
      "Train Epoch: 3 [4480/25569 (18%)]\tLoss: 0.180686\n",
      "Train Epoch: 3 [5120/25569 (20%)]\tLoss: 0.318703\n",
      "Train Epoch: 3 [5760/25569 (22%)]\tLoss: 0.191717\n",
      "Train Epoch: 3 [6400/25569 (25%)]\tLoss: 0.149804\n",
      "Train Epoch: 3 [7040/25569 (28%)]\tLoss: 0.320278\n",
      "Train Epoch: 3 [7680/25569 (30%)]\tLoss: 0.276184\n",
      "Train Epoch: 3 [8320/25569 (32%)]\tLoss: 0.279994\n",
      "Train Epoch: 3 [8960/25569 (35%)]\tLoss: 0.185190\n",
      "Train Epoch: 3 [9600/25569 (38%)]\tLoss: 0.274759\n",
      "Train Epoch: 3 [10240/25569 (40%)]\tLoss: 0.235802\n",
      "Train Epoch: 3 [10880/25569 (42%)]\tLoss: 0.267887\n",
      "Train Epoch: 3 [11520/25569 (45%)]\tLoss: 0.233231\n",
      "Train Epoch: 3 [12160/25569 (48%)]\tLoss: 0.285064\n",
      "Train Epoch: 3 [12800/25569 (50%)]\tLoss: 0.186670\n",
      "Train Epoch: 3 [13440/25569 (52%)]\tLoss: 0.154923\n",
      "Train Epoch: 3 [14080/25569 (55%)]\tLoss: 0.266367\n",
      "Train Epoch: 3 [14720/25569 (58%)]\tLoss: 0.193622\n",
      "Train Epoch: 3 [15360/25569 (60%)]\tLoss: 0.278177\n",
      "Train Epoch: 3 [16000/25569 (62%)]\tLoss: 0.387261\n",
      "Train Epoch: 3 [16640/25569 (65%)]\tLoss: 0.238442\n",
      "Train Epoch: 3 [17280/25569 (68%)]\tLoss: 0.275834\n",
      "Train Epoch: 3 [17920/25569 (70%)]\tLoss: 0.225033\n",
      "Train Epoch: 3 [18560/25569 (72%)]\tLoss: 0.269494\n",
      "Train Epoch: 3 [19200/25569 (75%)]\tLoss: 0.229530\n",
      "Train Epoch: 3 [19840/25569 (78%)]\tLoss: 0.191962\n",
      "Train Epoch: 3 [20480/25569 (80%)]\tLoss: 0.143836\n",
      "Train Epoch: 3 [21120/25569 (82%)]\tLoss: 0.255321\n",
      "Train Epoch: 3 [21760/25569 (85%)]\tLoss: 0.281381\n",
      "Train Epoch: 3 [22400/25569 (88%)]\tLoss: 0.376579\n",
      "Train Epoch: 3 [23040/25569 (90%)]\tLoss: 0.115359\n",
      "Train Epoch: 3 [23680/25569 (92%)]\tLoss: 0.205302\n",
      "Train Epoch: 3 [24320/25569 (95%)]\tLoss: 0.200141\n",
      "Train Epoch: 3 [24960/25569 (98%)]\tLoss: 0.186847\n",
      "\n",
      "Test set: Average loss: 0.2601, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/25569 (0%)]\tLoss: 0.287316\n",
      "Train Epoch: 4 [640/25569 (2%)]\tLoss: 0.194693\n",
      "Train Epoch: 4 [1280/25569 (5%)]\tLoss: 0.233676\n",
      "Train Epoch: 4 [1920/25569 (8%)]\tLoss: 0.280220\n",
      "Train Epoch: 4 [2560/25569 (10%)]\tLoss: 0.306710\n",
      "Train Epoch: 4 [3200/25569 (12%)]\tLoss: 0.399814\n",
      "Train Epoch: 4 [3840/25569 (15%)]\tLoss: 0.329111\n",
      "Train Epoch: 4 [4480/25569 (18%)]\tLoss: 0.308037\n",
      "Train Epoch: 4 [5120/25569 (20%)]\tLoss: 0.157292\n",
      "Train Epoch: 4 [5760/25569 (22%)]\tLoss: 0.391919\n",
      "Train Epoch: 4 [6400/25569 (25%)]\tLoss: 0.348246\n",
      "Train Epoch: 4 [7040/25569 (28%)]\tLoss: 0.114295\n",
      "Train Epoch: 4 [7680/25569 (30%)]\tLoss: 0.189787\n",
      "Train Epoch: 4 [8320/25569 (32%)]\tLoss: 0.275482\n",
      "Train Epoch: 4 [8960/25569 (35%)]\tLoss: 0.238050\n",
      "Train Epoch: 4 [9600/25569 (38%)]\tLoss: 0.227663\n",
      "Train Epoch: 4 [10240/25569 (40%)]\tLoss: 0.270335\n",
      "Train Epoch: 4 [10880/25569 (42%)]\tLoss: 0.189332\n",
      "Train Epoch: 4 [11520/25569 (45%)]\tLoss: 0.191662\n",
      "Train Epoch: 4 [12160/25569 (48%)]\tLoss: 0.151112\n",
      "Train Epoch: 4 [12800/25569 (50%)]\tLoss: 0.235284\n",
      "Train Epoch: 4 [13440/25569 (52%)]\tLoss: 0.302373\n",
      "Train Epoch: 4 [14080/25569 (55%)]\tLoss: 0.196824\n",
      "Train Epoch: 4 [14720/25569 (58%)]\tLoss: 0.100553\n",
      "Train Epoch: 4 [15360/25569 (60%)]\tLoss: 0.247256\n",
      "Train Epoch: 4 [16000/25569 (62%)]\tLoss: 0.285835\n",
      "Train Epoch: 4 [16640/25569 (65%)]\tLoss: 0.123567\n",
      "Train Epoch: 4 [17280/25569 (68%)]\tLoss: 0.107287\n",
      "Train Epoch: 4 [17920/25569 (70%)]\tLoss: 0.232143\n",
      "Train Epoch: 4 [18560/25569 (72%)]\tLoss: 0.356576\n",
      "Train Epoch: 4 [19200/25569 (75%)]\tLoss: 0.274438\n",
      "Train Epoch: 4 [19840/25569 (78%)]\tLoss: 0.317647\n",
      "Train Epoch: 4 [20480/25569 (80%)]\tLoss: 0.301063\n",
      "Train Epoch: 4 [21120/25569 (82%)]\tLoss: 0.275903\n",
      "Train Epoch: 4 [21760/25569 (85%)]\tLoss: 0.291775\n",
      "Train Epoch: 4 [22400/25569 (88%)]\tLoss: 0.277656\n",
      "Train Epoch: 4 [23040/25569 (90%)]\tLoss: 0.205572\n",
      "Train Epoch: 4 [23680/25569 (92%)]\tLoss: 0.331988\n",
      "Train Epoch: 4 [24320/25569 (95%)]\tLoss: 0.316287\n",
      "Train Epoch: 4 [24960/25569 (98%)]\tLoss: 0.154168\n",
      "\n",
      "Test set: Average loss: 0.2584, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/25569 (0%)]\tLoss: 0.233365\n",
      "Train Epoch: 5 [640/25569 (2%)]\tLoss: 0.506765\n",
      "Train Epoch: 5 [1280/25569 (5%)]\tLoss: 0.203604\n",
      "Train Epoch: 5 [1920/25569 (8%)]\tLoss: 0.239089\n",
      "Train Epoch: 5 [2560/25569 (10%)]\tLoss: 0.140046\n",
      "Train Epoch: 5 [3200/25569 (12%)]\tLoss: 0.317275\n",
      "Train Epoch: 5 [3840/25569 (15%)]\tLoss: 0.245865\n",
      "Train Epoch: 5 [4480/25569 (18%)]\tLoss: 0.238988\n",
      "Train Epoch: 5 [5120/25569 (20%)]\tLoss: 0.120745\n",
      "Train Epoch: 5 [5760/25569 (22%)]\tLoss: 0.188302\n",
      "Train Epoch: 5 [6400/25569 (25%)]\tLoss: 0.188120\n",
      "Train Epoch: 5 [7040/25569 (28%)]\tLoss: 0.273303\n",
      "Train Epoch: 5 [7680/25569 (30%)]\tLoss: 0.353321\n",
      "Train Epoch: 5 [8320/25569 (32%)]\tLoss: 0.354147\n",
      "Train Epoch: 5 [8960/25569 (35%)]\tLoss: 0.269520\n",
      "Train Epoch: 5 [9600/25569 (38%)]\tLoss: 0.273485\n",
      "Train Epoch: 5 [10240/25569 (40%)]\tLoss: 0.200896\n",
      "Train Epoch: 5 [10880/25569 (42%)]\tLoss: 0.199302\n",
      "Train Epoch: 5 [11520/25569 (45%)]\tLoss: 0.149379\n",
      "Train Epoch: 5 [12160/25569 (48%)]\tLoss: 0.273959\n",
      "Train Epoch: 5 [12800/25569 (50%)]\tLoss: 0.204996\n",
      "Train Epoch: 5 [13440/25569 (52%)]\tLoss: 0.232497\n",
      "Train Epoch: 5 [14080/25569 (55%)]\tLoss: 0.425220\n",
      "Train Epoch: 5 [14720/25569 (58%)]\tLoss: 0.354356\n",
      "Train Epoch: 5 [15360/25569 (60%)]\tLoss: 0.151738\n",
      "Train Epoch: 5 [16000/25569 (62%)]\tLoss: 0.186531\n",
      "Train Epoch: 5 [16640/25569 (65%)]\tLoss: 0.235577\n",
      "Train Epoch: 5 [17280/25569 (68%)]\tLoss: 0.357830\n",
      "Train Epoch: 5 [17920/25569 (70%)]\tLoss: 0.270176\n",
      "Train Epoch: 5 [18560/25569 (72%)]\tLoss: 0.161093\n",
      "Train Epoch: 5 [19200/25569 (75%)]\tLoss: 0.311821\n",
      "Train Epoch: 5 [19840/25569 (78%)]\tLoss: 0.325404\n",
      "Train Epoch: 5 [20480/25569 (80%)]\tLoss: 0.204125\n",
      "Train Epoch: 5 [21120/25569 (82%)]\tLoss: 0.246679\n",
      "Train Epoch: 5 [21760/25569 (85%)]\tLoss: 0.272454\n",
      "Train Epoch: 5 [22400/25569 (88%)]\tLoss: 0.302621\n",
      "Train Epoch: 5 [23040/25569 (90%)]\tLoss: 0.391011\n",
      "Train Epoch: 5 [23680/25569 (92%)]\tLoss: 0.196770\n",
      "Train Epoch: 5 [24320/25569 (95%)]\tLoss: 0.231594\n",
      "Train Epoch: 5 [24960/25569 (98%)]\tLoss: 0.278204\n",
      "\n",
      "Test set: Average loss: 0.2598, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 6 [0/25569 (0%)]\tLoss: 0.147036\n",
      "Train Epoch: 6 [640/25569 (2%)]\tLoss: 0.316569\n",
      "Train Epoch: 6 [1280/25569 (5%)]\tLoss: 0.269937\n",
      "Train Epoch: 6 [1920/25569 (8%)]\tLoss: 0.157928\n",
      "Train Epoch: 6 [2560/25569 (10%)]\tLoss: 0.240184\n",
      "Train Epoch: 6 [3200/25569 (12%)]\tLoss: 0.152840\n",
      "Train Epoch: 6 [3840/25569 (15%)]\tLoss: 0.162004\n",
      "Train Epoch: 6 [4480/25569 (18%)]\tLoss: 0.276373\n",
      "Train Epoch: 6 [5120/25569 (20%)]\tLoss: 0.281073\n",
      "Train Epoch: 6 [5760/25569 (22%)]\tLoss: 0.316108\n",
      "Train Epoch: 6 [6400/25569 (25%)]\tLoss: 0.196824\n",
      "Train Epoch: 6 [7040/25569 (28%)]\tLoss: 0.313922\n",
      "Train Epoch: 6 [7680/25569 (30%)]\tLoss: 0.343228\n",
      "Train Epoch: 6 [8320/25569 (32%)]\tLoss: 0.235677\n",
      "Train Epoch: 6 [8960/25569 (35%)]\tLoss: 0.194196\n",
      "Train Epoch: 6 [9600/25569 (38%)]\tLoss: 0.391077\n",
      "Train Epoch: 6 [10240/25569 (40%)]\tLoss: 0.107348\n",
      "Train Epoch: 6 [10880/25569 (42%)]\tLoss: 0.355352\n",
      "Train Epoch: 6 [11520/25569 (45%)]\tLoss: 0.163222\n",
      "Train Epoch: 6 [12160/25569 (48%)]\tLoss: 0.232231\n",
      "Train Epoch: 6 [12800/25569 (50%)]\tLoss: 0.142902\n",
      "Train Epoch: 6 [13440/25569 (52%)]\tLoss: 0.104940\n",
      "Train Epoch: 6 [14080/25569 (55%)]\tLoss: 0.149621\n",
      "Train Epoch: 6 [14720/25569 (58%)]\tLoss: 0.234650\n",
      "Train Epoch: 6 [15360/25569 (60%)]\tLoss: 0.236275\n",
      "Train Epoch: 6 [16000/25569 (62%)]\tLoss: 0.309921\n",
      "Train Epoch: 6 [16640/25569 (65%)]\tLoss: 0.139328\n",
      "Train Epoch: 6 [17280/25569 (68%)]\tLoss: 0.229691\n",
      "Train Epoch: 6 [17920/25569 (70%)]\tLoss: 0.321804\n",
      "Train Epoch: 6 [18560/25569 (72%)]\tLoss: 0.326017\n",
      "Train Epoch: 6 [19200/25569 (75%)]\tLoss: 0.193071\n",
      "Train Epoch: 6 [19840/25569 (78%)]\tLoss: 0.362684\n",
      "Train Epoch: 6 [20480/25569 (80%)]\tLoss: 0.201247\n",
      "Train Epoch: 6 [21120/25569 (82%)]\tLoss: 0.311726\n",
      "Train Epoch: 6 [21760/25569 (85%)]\tLoss: 0.316992\n",
      "Train Epoch: 6 [22400/25569 (88%)]\tLoss: 0.196734\n",
      "Train Epoch: 6 [23040/25569 (90%)]\tLoss: 0.342322\n",
      "Train Epoch: 6 [23680/25569 (92%)]\tLoss: 0.235909\n",
      "Train Epoch: 6 [24320/25569 (95%)]\tLoss: 0.277194\n",
      "Train Epoch: 6 [24960/25569 (98%)]\tLoss: 0.118443\n",
      "\n",
      "Test set: Average loss: 0.2580, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 7 [0/25569 (0%)]\tLoss: 0.192974\n",
      "Train Epoch: 7 [640/25569 (2%)]\tLoss: 0.283170\n",
      "Train Epoch: 7 [1280/25569 (5%)]\tLoss: 0.278068\n",
      "Train Epoch: 7 [1920/25569 (8%)]\tLoss: 0.239499\n",
      "Train Epoch: 7 [2560/25569 (10%)]\tLoss: 0.199336\n",
      "Train Epoch: 7 [3200/25569 (12%)]\tLoss: 0.154690\n",
      "Train Epoch: 7 [3840/25569 (15%)]\tLoss: 0.277323\n",
      "Train Epoch: 7 [4480/25569 (18%)]\tLoss: 0.198135\n",
      "Train Epoch: 7 [5120/25569 (20%)]\tLoss: 0.234439\n",
      "Train Epoch: 7 [5760/25569 (22%)]\tLoss: 0.196655\n",
      "Train Epoch: 7 [6400/25569 (25%)]\tLoss: 0.270342\n",
      "Train Epoch: 7 [7040/25569 (28%)]\tLoss: 0.153745\n",
      "Train Epoch: 7 [7680/25569 (30%)]\tLoss: 0.287841\n",
      "Train Epoch: 7 [8320/25569 (32%)]\tLoss: 0.155307\n",
      "Train Epoch: 7 [8960/25569 (35%)]\tLoss: 0.390211\n",
      "Train Epoch: 7 [9600/25569 (38%)]\tLoss: 0.239119\n",
      "Train Epoch: 7 [10240/25569 (40%)]\tLoss: 0.243209\n",
      "Train Epoch: 7 [10880/25569 (42%)]\tLoss: 0.116592\n",
      "Train Epoch: 7 [11520/25569 (45%)]\tLoss: 0.351287\n",
      "Train Epoch: 7 [12160/25569 (48%)]\tLoss: 0.466280\n",
      "Train Epoch: 7 [12800/25569 (50%)]\tLoss: 0.311218\n",
      "Train Epoch: 7 [13440/25569 (52%)]\tLoss: 0.314409\n",
      "Train Epoch: 7 [14080/25569 (55%)]\tLoss: 0.349319\n",
      "Train Epoch: 7 [14720/25569 (58%)]\tLoss: 0.148016\n",
      "Train Epoch: 7 [15360/25569 (60%)]\tLoss: 0.194831\n",
      "Train Epoch: 7 [16000/25569 (62%)]\tLoss: 0.158730\n",
      "Train Epoch: 7 [16640/25569 (65%)]\tLoss: 0.189849\n",
      "Train Epoch: 7 [17280/25569 (68%)]\tLoss: 0.239863\n",
      "Train Epoch: 7 [17920/25569 (70%)]\tLoss: 0.234261\n",
      "Train Epoch: 7 [18560/25569 (72%)]\tLoss: 0.197780\n",
      "Train Epoch: 7 [19200/25569 (75%)]\tLoss: 0.192647\n",
      "Train Epoch: 7 [19840/25569 (78%)]\tLoss: 0.413657\n",
      "Train Epoch: 7 [20480/25569 (80%)]\tLoss: 0.423552\n",
      "Train Epoch: 7 [21120/25569 (82%)]\tLoss: 0.358454\n",
      "Train Epoch: 7 [21760/25569 (85%)]\tLoss: 0.372247\n",
      "Train Epoch: 7 [22400/25569 (88%)]\tLoss: 0.190130\n",
      "Train Epoch: 7 [23040/25569 (90%)]\tLoss: 0.154111\n",
      "Train Epoch: 7 [23680/25569 (92%)]\tLoss: 0.320855\n",
      "Train Epoch: 7 [24320/25569 (95%)]\tLoss: 0.277434\n",
      "Train Epoch: 7 [24960/25569 (98%)]\tLoss: 0.352991\n",
      "\n",
      "Test set: Average loss: 0.2582, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 8 [0/25569 (0%)]\tLoss: 0.307406\n",
      "Train Epoch: 8 [640/25569 (2%)]\tLoss: 0.156527\n",
      "Train Epoch: 8 [1280/25569 (5%)]\tLoss: 0.361638\n",
      "Train Epoch: 8 [1920/25569 (8%)]\tLoss: 0.077974\n",
      "Train Epoch: 8 [2560/25569 (10%)]\tLoss: 0.197797\n",
      "Train Epoch: 8 [3200/25569 (12%)]\tLoss: 0.274781\n",
      "Train Epoch: 8 [3840/25569 (15%)]\tLoss: 0.357000\n",
      "Train Epoch: 8 [4480/25569 (18%)]\tLoss: 0.367814\n",
      "Train Epoch: 8 [5120/25569 (20%)]\tLoss: 0.315275\n",
      "Train Epoch: 8 [5760/25569 (22%)]\tLoss: 0.315953\n",
      "Train Epoch: 8 [6400/25569 (25%)]\tLoss: 0.230172\n",
      "Train Epoch: 8 [7040/25569 (28%)]\tLoss: 0.275230\n",
      "Train Epoch: 8 [7680/25569 (30%)]\tLoss: 0.308996\n",
      "Train Epoch: 8 [8320/25569 (32%)]\tLoss: 0.199411\n",
      "Train Epoch: 8 [8960/25569 (35%)]\tLoss: 0.242703\n",
      "Train Epoch: 8 [9600/25569 (38%)]\tLoss: 0.187997\n",
      "Train Epoch: 8 [10240/25569 (40%)]\tLoss: 0.145207\n",
      "Train Epoch: 8 [10880/25569 (42%)]\tLoss: 0.435702\n",
      "Train Epoch: 8 [11520/25569 (45%)]\tLoss: 0.316543\n",
      "Train Epoch: 8 [12160/25569 (48%)]\tLoss: 0.148162\n",
      "Train Epoch: 8 [12800/25569 (50%)]\tLoss: 0.196273\n",
      "Train Epoch: 8 [13440/25569 (52%)]\tLoss: 0.156447\n",
      "Train Epoch: 8 [14080/25569 (55%)]\tLoss: 0.189666\n",
      "Train Epoch: 8 [14720/25569 (58%)]\tLoss: 0.214972\n",
      "Train Epoch: 8 [15360/25569 (60%)]\tLoss: 0.158819\n",
      "Train Epoch: 8 [16000/25569 (62%)]\tLoss: 0.156436\n",
      "Train Epoch: 8 [16640/25569 (65%)]\tLoss: 0.146804\n",
      "Train Epoch: 8 [17280/25569 (68%)]\tLoss: 0.285459\n",
      "Train Epoch: 8 [17920/25569 (70%)]\tLoss: 0.220041\n",
      "Train Epoch: 8 [18560/25569 (72%)]\tLoss: 0.274032\n",
      "Train Epoch: 8 [19200/25569 (75%)]\tLoss: 0.168269\n",
      "Train Epoch: 8 [19840/25569 (78%)]\tLoss: 0.300393\n",
      "Train Epoch: 8 [20480/25569 (80%)]\tLoss: 0.326530\n",
      "Train Epoch: 8 [21120/25569 (82%)]\tLoss: 0.141412\n",
      "Train Epoch: 8 [21760/25569 (85%)]\tLoss: 0.186622\n",
      "Train Epoch: 8 [22400/25569 (88%)]\tLoss: 0.210893\n",
      "Train Epoch: 8 [23040/25569 (90%)]\tLoss: 0.209881\n",
      "Train Epoch: 8 [23680/25569 (92%)]\tLoss: 0.106657\n",
      "Train Epoch: 8 [24320/25569 (95%)]\tLoss: 0.168951\n",
      "Train Epoch: 8 [24960/25569 (98%)]\tLoss: 0.378603\n",
      "\n",
      "Test set: Average loss: 0.2407, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 9 [0/25569 (0%)]\tLoss: 0.191582\n",
      "Train Epoch: 9 [640/25569 (2%)]\tLoss: 0.227640\n",
      "Train Epoch: 9 [1280/25569 (5%)]\tLoss: 0.283632\n",
      "Train Epoch: 9 [1920/25569 (8%)]\tLoss: 0.182815\n",
      "Train Epoch: 9 [2560/25569 (10%)]\tLoss: 0.234802\n",
      "Train Epoch: 9 [3200/25569 (12%)]\tLoss: 0.203106\n",
      "Train Epoch: 9 [3840/25569 (15%)]\tLoss: 0.119926\n",
      "Train Epoch: 9 [4480/25569 (18%)]\tLoss: 0.236166\n",
      "Train Epoch: 9 [5120/25569 (20%)]\tLoss: 0.258953\n",
      "Train Epoch: 9 [5760/25569 (22%)]\tLoss: 0.155053\n",
      "Train Epoch: 9 [6400/25569 (25%)]\tLoss: 0.232888\n",
      "Train Epoch: 9 [7040/25569 (28%)]\tLoss: 0.126374\n",
      "Train Epoch: 9 [7680/25569 (30%)]\tLoss: 0.130732\n",
      "Train Epoch: 9 [8320/25569 (32%)]\tLoss: 0.305091\n",
      "Train Epoch: 9 [8960/25569 (35%)]\tLoss: 0.171954\n",
      "Train Epoch: 9 [9600/25569 (38%)]\tLoss: 0.213064\n",
      "Train Epoch: 9 [10240/25569 (40%)]\tLoss: 0.241875\n",
      "Train Epoch: 9 [10880/25569 (42%)]\tLoss: 0.167600\n",
      "Train Epoch: 9 [11520/25569 (45%)]\tLoss: 0.163871\n",
      "Train Epoch: 9 [12160/25569 (48%)]\tLoss: 0.203166\n",
      "Train Epoch: 9 [12800/25569 (50%)]\tLoss: 0.149155\n",
      "Train Epoch: 9 [13440/25569 (52%)]\tLoss: 0.177434\n",
      "Train Epoch: 9 [14080/25569 (55%)]\tLoss: 0.218556\n",
      "Train Epoch: 9 [14720/25569 (58%)]\tLoss: 0.388040\n",
      "Train Epoch: 9 [15360/25569 (60%)]\tLoss: 0.250141\n",
      "Train Epoch: 9 [16000/25569 (62%)]\tLoss: 0.277608\n",
      "Train Epoch: 9 [16640/25569 (65%)]\tLoss: 0.218919\n",
      "Train Epoch: 9 [17280/25569 (68%)]\tLoss: 0.327236\n",
      "Train Epoch: 9 [17920/25569 (70%)]\tLoss: 0.293845\n",
      "Train Epoch: 9 [18560/25569 (72%)]\tLoss: 0.194898\n",
      "Train Epoch: 9 [19200/25569 (75%)]\tLoss: 0.177718\n",
      "Train Epoch: 9 [19840/25569 (78%)]\tLoss: 0.102518\n",
      "Train Epoch: 9 [20480/25569 (80%)]\tLoss: 0.189987\n",
      "Train Epoch: 9 [21120/25569 (82%)]\tLoss: 0.111691\n",
      "Train Epoch: 9 [21760/25569 (85%)]\tLoss: 0.163146\n",
      "Train Epoch: 9 [22400/25569 (88%)]\tLoss: 0.349021\n",
      "Train Epoch: 9 [23040/25569 (90%)]\tLoss: 0.203334\n",
      "Train Epoch: 9 [23680/25569 (92%)]\tLoss: 0.167039\n",
      "Train Epoch: 9 [24320/25569 (95%)]\tLoss: 0.143467\n",
      "Train Epoch: 9 [24960/25569 (98%)]\tLoss: 0.182922\n",
      "\n",
      "Test set: Average loss: 0.2124, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 10 [0/25569 (0%)]\tLoss: 0.091846\n",
      "Train Epoch: 10 [640/25569 (2%)]\tLoss: 0.198296\n",
      "Train Epoch: 10 [1280/25569 (5%)]\tLoss: 0.201740\n",
      "Train Epoch: 10 [1920/25569 (8%)]\tLoss: 0.197725\n",
      "Train Epoch: 10 [2560/25569 (10%)]\tLoss: 0.229620\n",
      "Train Epoch: 10 [3200/25569 (12%)]\tLoss: 0.236989\n",
      "Train Epoch: 10 [3840/25569 (15%)]\tLoss: 0.170984\n",
      "Train Epoch: 10 [4480/25569 (18%)]\tLoss: 0.111121\n",
      "Train Epoch: 10 [5120/25569 (20%)]\tLoss: 0.103060\n",
      "Train Epoch: 10 [5760/25569 (22%)]\tLoss: 0.113623\n",
      "Train Epoch: 10 [6400/25569 (25%)]\tLoss: 0.197920\n",
      "Train Epoch: 10 [7040/25569 (28%)]\tLoss: 0.188884\n",
      "Train Epoch: 10 [7680/25569 (30%)]\tLoss: 0.223252\n",
      "Train Epoch: 10 [8320/25569 (32%)]\tLoss: 0.103742\n",
      "Train Epoch: 10 [8960/25569 (35%)]\tLoss: 0.150015\n",
      "Train Epoch: 10 [9600/25569 (38%)]\tLoss: 0.297084\n",
      "Train Epoch: 10 [10240/25569 (40%)]\tLoss: 0.224629\n",
      "Train Epoch: 10 [10880/25569 (42%)]\tLoss: 0.157414\n",
      "Train Epoch: 10 [11520/25569 (45%)]\tLoss: 0.333774\n",
      "Train Epoch: 10 [12160/25569 (48%)]\tLoss: 0.263845\n",
      "Train Epoch: 10 [12800/25569 (50%)]\tLoss: 0.336255\n",
      "Train Epoch: 10 [13440/25569 (52%)]\tLoss: 0.146836\n",
      "Train Epoch: 10 [14080/25569 (55%)]\tLoss: 0.253503\n",
      "Train Epoch: 10 [14720/25569 (58%)]\tLoss: 0.284904\n",
      "Train Epoch: 10 [15360/25569 (60%)]\tLoss: 0.235329\n",
      "Train Epoch: 10 [16000/25569 (62%)]\tLoss: 0.166408\n",
      "Train Epoch: 10 [16640/25569 (65%)]\tLoss: 0.169638\n",
      "Train Epoch: 10 [17280/25569 (68%)]\tLoss: 0.219532\n",
      "Train Epoch: 10 [17920/25569 (70%)]\tLoss: 0.209403\n",
      "Train Epoch: 10 [18560/25569 (72%)]\tLoss: 0.218115\n",
      "Train Epoch: 10 [19200/25569 (75%)]\tLoss: 0.068903\n",
      "Train Epoch: 10 [19840/25569 (78%)]\tLoss: 0.235268\n",
      "Train Epoch: 10 [20480/25569 (80%)]\tLoss: 0.084157\n",
      "Train Epoch: 10 [21120/25569 (82%)]\tLoss: 0.286852\n",
      "Train Epoch: 10 [21760/25569 (85%)]\tLoss: 0.168099\n",
      "Train Epoch: 10 [22400/25569 (88%)]\tLoss: 0.270399\n",
      "Train Epoch: 10 [23040/25569 (90%)]\tLoss: 0.150984\n",
      "Train Epoch: 10 [23680/25569 (92%)]\tLoss: 0.281284\n",
      "Train Epoch: 10 [24320/25569 (95%)]\tLoss: 0.278407\n",
      "Train Epoch: 10 [24960/25569 (98%)]\tLoss: 0.134499\n",
      "\n",
      "Test set: Average loss: 0.2099, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 11 [0/25569 (0%)]\tLoss: 0.218589\n",
      "Train Epoch: 11 [640/25569 (2%)]\tLoss: 0.100510\n",
      "Train Epoch: 11 [1280/25569 (5%)]\tLoss: 0.192514\n",
      "Train Epoch: 11 [1920/25569 (8%)]\tLoss: 0.117091\n",
      "Train Epoch: 11 [2560/25569 (10%)]\tLoss: 0.165489\n",
      "Train Epoch: 11 [3200/25569 (12%)]\tLoss: 0.180157\n",
      "Train Epoch: 11 [3840/25569 (15%)]\tLoss: 0.054047\n",
      "Train Epoch: 11 [4480/25569 (18%)]\tLoss: 0.127531\n",
      "Train Epoch: 11 [5120/25569 (20%)]\tLoss: 0.202046\n",
      "Train Epoch: 11 [5760/25569 (22%)]\tLoss: 0.275185\n",
      "Train Epoch: 11 [6400/25569 (25%)]\tLoss: 0.161517\n",
      "Train Epoch: 11 [7040/25569 (28%)]\tLoss: 0.182755\n",
      "Train Epoch: 11 [7680/25569 (30%)]\tLoss: 0.407702\n",
      "Train Epoch: 11 [8320/25569 (32%)]\tLoss: 0.316638\n",
      "Train Epoch: 11 [8960/25569 (35%)]\tLoss: 0.173498\n",
      "Train Epoch: 11 [9600/25569 (38%)]\tLoss: 0.226715\n",
      "Train Epoch: 11 [10240/25569 (40%)]\tLoss: 0.153854\n",
      "Train Epoch: 11 [10880/25569 (42%)]\tLoss: 0.139483\n",
      "Train Epoch: 11 [11520/25569 (45%)]\tLoss: 0.213169\n",
      "Train Epoch: 11 [12160/25569 (48%)]\tLoss: 0.221021\n",
      "Train Epoch: 11 [12800/25569 (50%)]\tLoss: 0.225768\n",
      "Train Epoch: 11 [13440/25569 (52%)]\tLoss: 0.250992\n",
      "Train Epoch: 11 [14080/25569 (55%)]\tLoss: 0.159246\n",
      "Train Epoch: 11 [14720/25569 (58%)]\tLoss: 0.197821\n",
      "Train Epoch: 11 [15360/25569 (60%)]\tLoss: 0.206735\n",
      "Train Epoch: 11 [16000/25569 (62%)]\tLoss: 0.195600\n",
      "Train Epoch: 11 [16640/25569 (65%)]\tLoss: 0.191383\n",
      "Train Epoch: 11 [17280/25569 (68%)]\tLoss: 0.263111\n",
      "Train Epoch: 11 [17920/25569 (70%)]\tLoss: 0.233759\n",
      "Train Epoch: 11 [18560/25569 (72%)]\tLoss: 0.178417\n",
      "Train Epoch: 11 [19200/25569 (75%)]\tLoss: 0.333567\n",
      "Train Epoch: 11 [19840/25569 (78%)]\tLoss: 0.239125\n",
      "Train Epoch: 11 [20480/25569 (80%)]\tLoss: 0.149021\n",
      "Train Epoch: 11 [21120/25569 (82%)]\tLoss: 0.205815\n",
      "Train Epoch: 11 [21760/25569 (85%)]\tLoss: 0.169613\n",
      "Train Epoch: 11 [22400/25569 (88%)]\tLoss: 0.186995\n",
      "Train Epoch: 11 [23040/25569 (90%)]\tLoss: 0.170628\n",
      "Train Epoch: 11 [23680/25569 (92%)]\tLoss: 0.190845\n",
      "Train Epoch: 11 [24320/25569 (95%)]\tLoss: 0.269719\n",
      "Train Epoch: 11 [24960/25569 (98%)]\tLoss: 0.232593\n",
      "\n",
      "Test set: Average loss: 0.2080, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 12 [0/25569 (0%)]\tLoss: 0.179404\n",
      "Train Epoch: 12 [640/25569 (2%)]\tLoss: 0.218385\n",
      "Train Epoch: 12 [1280/25569 (5%)]\tLoss: 0.178968\n",
      "Train Epoch: 12 [1920/25569 (8%)]\tLoss: 0.128839\n",
      "Train Epoch: 12 [2560/25569 (10%)]\tLoss: 0.138365\n",
      "Train Epoch: 12 [3200/25569 (12%)]\tLoss: 0.164233\n",
      "Train Epoch: 12 [3840/25569 (15%)]\tLoss: 0.135358\n",
      "Train Epoch: 12 [4480/25569 (18%)]\tLoss: 0.191003\n",
      "Train Epoch: 12 [5120/25569 (20%)]\tLoss: 0.170859\n",
      "Train Epoch: 12 [5760/25569 (22%)]\tLoss: 0.138082\n",
      "Train Epoch: 12 [6400/25569 (25%)]\tLoss: 0.239965\n",
      "Train Epoch: 12 [7040/25569 (28%)]\tLoss: 0.223587\n",
      "Train Epoch: 12 [7680/25569 (30%)]\tLoss: 0.348716\n",
      "Train Epoch: 12 [8320/25569 (32%)]\tLoss: 0.307843\n",
      "Train Epoch: 12 [8960/25569 (35%)]\tLoss: 0.219486\n",
      "Train Epoch: 12 [9600/25569 (38%)]\tLoss: 0.244962\n",
      "Train Epoch: 12 [10240/25569 (40%)]\tLoss: 0.140207\n",
      "Train Epoch: 12 [10880/25569 (42%)]\tLoss: 0.176563\n",
      "Train Epoch: 12 [11520/25569 (45%)]\tLoss: 0.146515\n",
      "Train Epoch: 12 [12160/25569 (48%)]\tLoss: 0.185957\n",
      "Train Epoch: 12 [12800/25569 (50%)]\tLoss: 0.223168\n",
      "Train Epoch: 12 [13440/25569 (52%)]\tLoss: 0.111602\n",
      "Train Epoch: 12 [14080/25569 (55%)]\tLoss: 0.078097\n",
      "Train Epoch: 12 [14720/25569 (58%)]\tLoss: 0.168212\n",
      "Train Epoch: 12 [15360/25569 (60%)]\tLoss: 0.120077\n",
      "Train Epoch: 12 [16000/25569 (62%)]\tLoss: 0.156564\n",
      "Train Epoch: 12 [16640/25569 (65%)]\tLoss: 0.399071\n",
      "Train Epoch: 12 [17280/25569 (68%)]\tLoss: 0.153500\n",
      "Train Epoch: 12 [17920/25569 (70%)]\tLoss: 0.195772\n",
      "Train Epoch: 12 [18560/25569 (72%)]\tLoss: 0.230901\n",
      "Train Epoch: 12 [19200/25569 (75%)]\tLoss: 0.137352\n",
      "Train Epoch: 12 [19840/25569 (78%)]\tLoss: 0.235352\n",
      "Train Epoch: 12 [20480/25569 (80%)]\tLoss: 0.238082\n",
      "Train Epoch: 12 [21120/25569 (82%)]\tLoss: 0.194252\n",
      "Train Epoch: 12 [21760/25569 (85%)]\tLoss: 0.148593\n",
      "Train Epoch: 12 [22400/25569 (88%)]\tLoss: 0.166824\n",
      "Train Epoch: 12 [23040/25569 (90%)]\tLoss: 0.178686\n",
      "Train Epoch: 12 [23680/25569 (92%)]\tLoss: 0.162866\n",
      "Train Epoch: 12 [24320/25569 (95%)]\tLoss: 0.174872\n",
      "Train Epoch: 12 [24960/25569 (98%)]\tLoss: 0.137473\n",
      "\n",
      "Test set: Average loss: 0.2072, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 13 [0/25569 (0%)]\tLoss: 0.179080\n",
      "Train Epoch: 13 [640/25569 (2%)]\tLoss: 0.239298\n",
      "Train Epoch: 13 [1280/25569 (5%)]\tLoss: 0.229929\n",
      "Train Epoch: 13 [1920/25569 (8%)]\tLoss: 0.113119\n",
      "Train Epoch: 13 [2560/25569 (10%)]\tLoss: 0.157295\n",
      "Train Epoch: 13 [3200/25569 (12%)]\tLoss: 0.233468\n",
      "Train Epoch: 13 [3840/25569 (15%)]\tLoss: 0.256462\n",
      "Train Epoch: 13 [4480/25569 (18%)]\tLoss: 0.181936\n",
      "Train Epoch: 13 [5120/25569 (20%)]\tLoss: 0.166381\n",
      "Train Epoch: 13 [5760/25569 (22%)]\tLoss: 0.099116\n",
      "Train Epoch: 13 [6400/25569 (25%)]\tLoss: 0.183130\n",
      "Train Epoch: 13 [7040/25569 (28%)]\tLoss: 0.083267\n",
      "Train Epoch: 13 [7680/25569 (30%)]\tLoss: 0.197656\n",
      "Train Epoch: 13 [8320/25569 (32%)]\tLoss: 0.253742\n",
      "Train Epoch: 13 [8960/25569 (35%)]\tLoss: 0.135235\n",
      "Train Epoch: 13 [9600/25569 (38%)]\tLoss: 0.222790\n",
      "Train Epoch: 13 [10240/25569 (40%)]\tLoss: 0.235232\n",
      "Train Epoch: 13 [10880/25569 (42%)]\tLoss: 0.178811\n",
      "Train Epoch: 13 [11520/25569 (45%)]\tLoss: 0.326047\n",
      "Train Epoch: 13 [12160/25569 (48%)]\tLoss: 0.172918\n",
      "Train Epoch: 13 [12800/25569 (50%)]\tLoss: 0.097757\n",
      "Train Epoch: 13 [13440/25569 (52%)]\tLoss: 0.291707\n",
      "Train Epoch: 13 [14080/25569 (55%)]\tLoss: 0.252034\n",
      "Train Epoch: 13 [14720/25569 (58%)]\tLoss: 0.342778\n",
      "Train Epoch: 13 [15360/25569 (60%)]\tLoss: 0.157120\n",
      "Train Epoch: 13 [16000/25569 (62%)]\tLoss: 0.176135\n",
      "Train Epoch: 13 [16640/25569 (65%)]\tLoss: 0.220060\n",
      "Train Epoch: 13 [17280/25569 (68%)]\tLoss: 0.155388\n",
      "Train Epoch: 13 [17920/25569 (70%)]\tLoss: 0.133087\n",
      "Train Epoch: 13 [18560/25569 (72%)]\tLoss: 0.234816\n",
      "Train Epoch: 13 [19200/25569 (75%)]\tLoss: 0.149920\n",
      "Train Epoch: 13 [19840/25569 (78%)]\tLoss: 0.217724\n",
      "Train Epoch: 13 [20480/25569 (80%)]\tLoss: 0.083779\n",
      "Train Epoch: 13 [21120/25569 (82%)]\tLoss: 0.184014\n",
      "Train Epoch: 13 [21760/25569 (85%)]\tLoss: 0.196376\n",
      "Train Epoch: 13 [22400/25569 (88%)]\tLoss: 0.234600\n",
      "Train Epoch: 13 [23040/25569 (90%)]\tLoss: 0.077241\n",
      "Train Epoch: 13 [23680/25569 (92%)]\tLoss: 0.131843\n",
      "Train Epoch: 13 [24320/25569 (95%)]\tLoss: 0.428925\n",
      "Train Epoch: 13 [24960/25569 (98%)]\tLoss: 0.174201\n",
      "\n",
      "Test set: Average loss: 0.2068, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 14 [0/25569 (0%)]\tLoss: 0.198526\n",
      "Train Epoch: 14 [640/25569 (2%)]\tLoss: 0.158672\n",
      "Train Epoch: 14 [1280/25569 (5%)]\tLoss: 0.307668\n",
      "Train Epoch: 14 [1920/25569 (8%)]\tLoss: 0.239331\n",
      "Train Epoch: 14 [2560/25569 (10%)]\tLoss: 0.211421\n",
      "Train Epoch: 14 [3200/25569 (12%)]\tLoss: 0.303022\n",
      "Train Epoch: 14 [3840/25569 (15%)]\tLoss: 0.291996\n",
      "Train Epoch: 14 [4480/25569 (18%)]\tLoss: 0.358881\n",
      "Train Epoch: 14 [5120/25569 (20%)]\tLoss: 0.261512\n",
      "Train Epoch: 14 [5760/25569 (22%)]\tLoss: 0.235935\n",
      "Train Epoch: 14 [6400/25569 (25%)]\tLoss: 0.069023\n",
      "Train Epoch: 14 [7040/25569 (28%)]\tLoss: 0.294997\n",
      "Train Epoch: 14 [7680/25569 (30%)]\tLoss: 0.269492\n",
      "Train Epoch: 14 [8320/25569 (32%)]\tLoss: 0.299964\n",
      "Train Epoch: 14 [8960/25569 (35%)]\tLoss: 0.207376\n",
      "Train Epoch: 14 [9600/25569 (38%)]\tLoss: 0.125129\n",
      "Train Epoch: 14 [10240/25569 (40%)]\tLoss: 0.090598\n",
      "Train Epoch: 14 [10880/25569 (42%)]\tLoss: 0.297385\n",
      "Train Epoch: 14 [11520/25569 (45%)]\tLoss: 0.264869\n",
      "Train Epoch: 14 [12160/25569 (48%)]\tLoss: 0.165178\n",
      "Train Epoch: 14 [12800/25569 (50%)]\tLoss: 0.183889\n",
      "Train Epoch: 14 [13440/25569 (52%)]\tLoss: 0.250306\n",
      "Train Epoch: 14 [14080/25569 (55%)]\tLoss: 0.174892\n",
      "Train Epoch: 14 [14720/25569 (58%)]\tLoss: 0.197229\n",
      "Train Epoch: 14 [15360/25569 (60%)]\tLoss: 0.202619\n",
      "Train Epoch: 14 [16000/25569 (62%)]\tLoss: 0.206245\n",
      "Train Epoch: 14 [16640/25569 (65%)]\tLoss: 0.357011\n",
      "Train Epoch: 14 [17280/25569 (68%)]\tLoss: 0.090861\n",
      "Train Epoch: 14 [17920/25569 (70%)]\tLoss: 0.334220\n",
      "Train Epoch: 14 [18560/25569 (72%)]\tLoss: 0.055302\n",
      "Train Epoch: 14 [19200/25569 (75%)]\tLoss: 0.148612\n",
      "Train Epoch: 14 [19840/25569 (78%)]\tLoss: 0.175426\n",
      "Train Epoch: 14 [20480/25569 (80%)]\tLoss: 0.151756\n",
      "Train Epoch: 14 [21120/25569 (82%)]\tLoss: 0.155216\n",
      "Train Epoch: 14 [21760/25569 (85%)]\tLoss: 0.115653\n",
      "Train Epoch: 14 [22400/25569 (88%)]\tLoss: 0.212330\n",
      "Train Epoch: 14 [23040/25569 (90%)]\tLoss: 0.226169\n",
      "Train Epoch: 14 [23680/25569 (92%)]\tLoss: 0.256771\n",
      "Train Epoch: 14 [24320/25569 (95%)]\tLoss: 0.209759\n",
      "Train Epoch: 14 [24960/25569 (98%)]\tLoss: 0.160266\n",
      "\n",
      "Test set: Average loss: 0.2062, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 15 [0/25569 (0%)]\tLoss: 0.311564\n",
      "Train Epoch: 15 [640/25569 (2%)]\tLoss: 0.234491\n",
      "Train Epoch: 15 [1280/25569 (5%)]\tLoss: 0.186584\n",
      "Train Epoch: 15 [1920/25569 (8%)]\tLoss: 0.268457\n",
      "Train Epoch: 15 [2560/25569 (10%)]\tLoss: 0.173985\n",
      "Train Epoch: 15 [3200/25569 (12%)]\tLoss: 0.172054\n",
      "Train Epoch: 15 [3840/25569 (15%)]\tLoss: 0.220749\n",
      "Train Epoch: 15 [4480/25569 (18%)]\tLoss: 0.292685\n",
      "Train Epoch: 15 [5120/25569 (20%)]\tLoss: 0.124672\n",
      "Train Epoch: 15 [5760/25569 (22%)]\tLoss: 0.326370\n",
      "Train Epoch: 15 [6400/25569 (25%)]\tLoss: 0.213327\n",
      "Train Epoch: 15 [7040/25569 (28%)]\tLoss: 0.206542\n",
      "Train Epoch: 15 [7680/25569 (30%)]\tLoss: 0.166246\n",
      "Train Epoch: 15 [8320/25569 (32%)]\tLoss: 0.138152\n",
      "Train Epoch: 15 [8960/25569 (35%)]\tLoss: 0.187753\n",
      "Train Epoch: 15 [9600/25569 (38%)]\tLoss: 0.221764\n",
      "Train Epoch: 15 [10240/25569 (40%)]\tLoss: 0.207380\n",
      "Train Epoch: 15 [10880/25569 (42%)]\tLoss: 0.219684\n",
      "Train Epoch: 15 [11520/25569 (45%)]\tLoss: 0.195279\n",
      "Train Epoch: 15 [12160/25569 (48%)]\tLoss: 0.138374\n",
      "Train Epoch: 15 [12800/25569 (50%)]\tLoss: 0.183240\n",
      "Train Epoch: 15 [13440/25569 (52%)]\tLoss: 0.253323\n",
      "Train Epoch: 15 [14080/25569 (55%)]\tLoss: 0.174625\n",
      "Train Epoch: 15 [14720/25569 (58%)]\tLoss: 0.399978\n",
      "Train Epoch: 15 [15360/25569 (60%)]\tLoss: 0.168243\n",
      "Train Epoch: 15 [16000/25569 (62%)]\tLoss: 0.304614\n",
      "Train Epoch: 15 [16640/25569 (65%)]\tLoss: 0.237020\n",
      "Train Epoch: 15 [17280/25569 (68%)]\tLoss: 0.285983\n",
      "Train Epoch: 15 [17920/25569 (70%)]\tLoss: 0.273519\n",
      "Train Epoch: 15 [18560/25569 (72%)]\tLoss: 0.303956\n",
      "Train Epoch: 15 [19200/25569 (75%)]\tLoss: 0.209981\n",
      "Train Epoch: 15 [19840/25569 (78%)]\tLoss: 0.165606\n",
      "Train Epoch: 15 [20480/25569 (80%)]\tLoss: 0.129688\n",
      "Train Epoch: 15 [21120/25569 (82%)]\tLoss: 0.120277\n",
      "Train Epoch: 15 [21760/25569 (85%)]\tLoss: 0.133409\n",
      "Train Epoch: 15 [22400/25569 (88%)]\tLoss: 0.201469\n",
      "Train Epoch: 15 [23040/25569 (90%)]\tLoss: 0.113620\n",
      "Train Epoch: 15 [23680/25569 (92%)]\tLoss: 0.186414\n",
      "Train Epoch: 15 [24320/25569 (95%)]\tLoss: 0.377622\n",
      "Train Epoch: 15 [24960/25569 (98%)]\tLoss: 0.179171\n",
      "\n",
      "Test set: Average loss: 0.2059, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 16 [0/25569 (0%)]\tLoss: 0.240182\n",
      "Train Epoch: 16 [640/25569 (2%)]\tLoss: 0.226717\n",
      "Train Epoch: 16 [1280/25569 (5%)]\tLoss: 0.195217\n",
      "Train Epoch: 16 [1920/25569 (8%)]\tLoss: 0.074031\n",
      "Train Epoch: 16 [2560/25569 (10%)]\tLoss: 0.232213\n",
      "Train Epoch: 16 [3200/25569 (12%)]\tLoss: 0.306910\n",
      "Train Epoch: 16 [3840/25569 (15%)]\tLoss: 0.246850\n",
      "Train Epoch: 16 [4480/25569 (18%)]\tLoss: 0.230953\n",
      "Train Epoch: 16 [5120/25569 (20%)]\tLoss: 0.211739\n",
      "Train Epoch: 16 [5760/25569 (22%)]\tLoss: 0.154601\n",
      "Train Epoch: 16 [6400/25569 (25%)]\tLoss: 0.205059\n",
      "Train Epoch: 16 [7040/25569 (28%)]\tLoss: 0.144391\n",
      "Train Epoch: 16 [7680/25569 (30%)]\tLoss: 0.221502\n",
      "Train Epoch: 16 [8320/25569 (32%)]\tLoss: 0.208691\n",
      "Train Epoch: 16 [8960/25569 (35%)]\tLoss: 0.220870\n",
      "Train Epoch: 16 [9600/25569 (38%)]\tLoss: 0.265065\n",
      "Train Epoch: 16 [10240/25569 (40%)]\tLoss: 0.345553\n",
      "Train Epoch: 16 [10880/25569 (42%)]\tLoss: 0.199502\n",
      "Train Epoch: 16 [11520/25569 (45%)]\tLoss: 0.229046\n",
      "Train Epoch: 16 [12160/25569 (48%)]\tLoss: 0.166492\n",
      "Train Epoch: 16 [12800/25569 (50%)]\tLoss: 0.408385\n",
      "Train Epoch: 16 [13440/25569 (52%)]\tLoss: 0.171344\n",
      "Train Epoch: 16 [14080/25569 (55%)]\tLoss: 0.219951\n",
      "Train Epoch: 16 [14720/25569 (58%)]\tLoss: 0.168407\n",
      "Train Epoch: 16 [15360/25569 (60%)]\tLoss: 0.252890\n",
      "Train Epoch: 16 [16000/25569 (62%)]\tLoss: 0.174671\n",
      "Train Epoch: 16 [16640/25569 (65%)]\tLoss: 0.305475\n",
      "Train Epoch: 16 [17280/25569 (68%)]\tLoss: 0.157127\n",
      "Train Epoch: 16 [17920/25569 (70%)]\tLoss: 0.168205\n",
      "Train Epoch: 16 [18560/25569 (72%)]\tLoss: 0.141347\n",
      "Train Epoch: 16 [19200/25569 (75%)]\tLoss: 0.220933\n",
      "Train Epoch: 16 [19840/25569 (78%)]\tLoss: 0.180680\n",
      "Train Epoch: 16 [20480/25569 (80%)]\tLoss: 0.246177\n",
      "Train Epoch: 16 [21120/25569 (82%)]\tLoss: 0.174464\n",
      "Train Epoch: 16 [21760/25569 (85%)]\tLoss: 0.245471\n",
      "Train Epoch: 16 [22400/25569 (88%)]\tLoss: 0.178682\n",
      "Train Epoch: 16 [23040/25569 (90%)]\tLoss: 0.210268\n",
      "Train Epoch: 16 [23680/25569 (92%)]\tLoss: 0.303720\n",
      "Train Epoch: 16 [24320/25569 (95%)]\tLoss: 0.126404\n",
      "Train Epoch: 16 [24960/25569 (98%)]\tLoss: 0.112273\n",
      "\n",
      "Test set: Average loss: 0.2051, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 17 [0/25569 (0%)]\tLoss: 0.240191\n",
      "Train Epoch: 17 [640/25569 (2%)]\tLoss: 0.187041\n",
      "Train Epoch: 17 [1280/25569 (5%)]\tLoss: 0.251669\n",
      "Train Epoch: 17 [1920/25569 (8%)]\tLoss: 0.247193\n",
      "Train Epoch: 17 [2560/25569 (10%)]\tLoss: 0.256241\n",
      "Train Epoch: 17 [3200/25569 (12%)]\tLoss: 0.232092\n",
      "Train Epoch: 17 [3840/25569 (15%)]\tLoss: 0.274571\n",
      "Train Epoch: 17 [4480/25569 (18%)]\tLoss: 0.225103\n",
      "Train Epoch: 17 [5120/25569 (20%)]\tLoss: 0.261724\n",
      "Train Epoch: 17 [5760/25569 (22%)]\tLoss: 0.183488\n",
      "Train Epoch: 17 [6400/25569 (25%)]\tLoss: 0.220206\n",
      "Train Epoch: 17 [7040/25569 (28%)]\tLoss: 0.163216\n",
      "Train Epoch: 17 [7680/25569 (30%)]\tLoss: 0.214796\n",
      "Train Epoch: 17 [8320/25569 (32%)]\tLoss: 0.225917\n",
      "Train Epoch: 17 [8960/25569 (35%)]\tLoss: 0.192666\n",
      "Train Epoch: 17 [9600/25569 (38%)]\tLoss: 0.339566\n",
      "Train Epoch: 17 [10240/25569 (40%)]\tLoss: 0.213338\n",
      "Train Epoch: 17 [10880/25569 (42%)]\tLoss: 0.121561\n",
      "Train Epoch: 17 [11520/25569 (45%)]\tLoss: 0.168532\n",
      "Train Epoch: 17 [12160/25569 (48%)]\tLoss: 0.243952\n",
      "Train Epoch: 17 [12800/25569 (50%)]\tLoss: 0.170453\n",
      "Train Epoch: 17 [13440/25569 (52%)]\tLoss: 0.177282\n",
      "Train Epoch: 17 [14080/25569 (55%)]\tLoss: 0.191128\n",
      "Train Epoch: 17 [14720/25569 (58%)]\tLoss: 0.186518\n",
      "Train Epoch: 17 [15360/25569 (60%)]\tLoss: 0.357979\n",
      "Train Epoch: 17 [16000/25569 (62%)]\tLoss: 0.246079\n",
      "Train Epoch: 17 [16640/25569 (65%)]\tLoss: 0.179153\n",
      "Train Epoch: 17 [17280/25569 (68%)]\tLoss: 0.117215\n",
      "Train Epoch: 17 [17920/25569 (70%)]\tLoss: 0.198397\n",
      "Train Epoch: 17 [18560/25569 (72%)]\tLoss: 0.146374\n",
      "Train Epoch: 17 [19200/25569 (75%)]\tLoss: 0.155924\n",
      "Train Epoch: 17 [19840/25569 (78%)]\tLoss: 0.161529\n",
      "Train Epoch: 17 [20480/25569 (80%)]\tLoss: 0.097526\n",
      "Train Epoch: 17 [21120/25569 (82%)]\tLoss: 0.189266\n",
      "Train Epoch: 17 [21760/25569 (85%)]\tLoss: 0.248248\n",
      "Train Epoch: 17 [22400/25569 (88%)]\tLoss: 0.240313\n",
      "Train Epoch: 17 [23040/25569 (90%)]\tLoss: 0.376527\n",
      "Train Epoch: 17 [23680/25569 (92%)]\tLoss: 0.178205\n",
      "Train Epoch: 17 [24320/25569 (95%)]\tLoss: 0.212462\n",
      "Train Epoch: 17 [24960/25569 (98%)]\tLoss: 0.323431\n",
      "\n",
      "Test set: Average loss: 0.2048, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 18 [0/25569 (0%)]\tLoss: 0.153245\n",
      "Train Epoch: 18 [640/25569 (2%)]\tLoss: 0.232526\n",
      "Train Epoch: 18 [1280/25569 (5%)]\tLoss: 0.152393\n",
      "Train Epoch: 18 [1920/25569 (8%)]\tLoss: 0.286077\n",
      "Train Epoch: 18 [2560/25569 (10%)]\tLoss: 0.260687\n",
      "Train Epoch: 18 [3200/25569 (12%)]\tLoss: 0.210040\n",
      "Train Epoch: 18 [3840/25569 (15%)]\tLoss: 0.271884\n",
      "Train Epoch: 18 [4480/25569 (18%)]\tLoss: 0.186490\n",
      "Train Epoch: 18 [5120/25569 (20%)]\tLoss: 0.119043\n",
      "Train Epoch: 18 [5760/25569 (22%)]\tLoss: 0.139061\n",
      "Train Epoch: 18 [6400/25569 (25%)]\tLoss: 0.189854\n",
      "Train Epoch: 18 [7040/25569 (28%)]\tLoss: 0.073800\n",
      "Train Epoch: 18 [7680/25569 (30%)]\tLoss: 0.264498\n",
      "Train Epoch: 18 [8320/25569 (32%)]\tLoss: 0.272619\n",
      "Train Epoch: 18 [8960/25569 (35%)]\tLoss: 0.216694\n",
      "Train Epoch: 18 [9600/25569 (38%)]\tLoss: 0.139767\n",
      "Train Epoch: 18 [10240/25569 (40%)]\tLoss: 0.291127\n",
      "Train Epoch: 18 [10880/25569 (42%)]\tLoss: 0.211351\n",
      "Train Epoch: 18 [11520/25569 (45%)]\tLoss: 0.230967\n",
      "Train Epoch: 18 [12160/25569 (48%)]\tLoss: 0.258697\n",
      "Train Epoch: 18 [12800/25569 (50%)]\tLoss: 0.121544\n",
      "Train Epoch: 18 [13440/25569 (52%)]\tLoss: 0.184613\n",
      "Train Epoch: 18 [14080/25569 (55%)]\tLoss: 0.224206\n",
      "Train Epoch: 18 [14720/25569 (58%)]\tLoss: 0.218830\n",
      "Train Epoch: 18 [15360/25569 (60%)]\tLoss: 0.141801\n",
      "Train Epoch: 18 [16000/25569 (62%)]\tLoss: 0.189788\n",
      "Train Epoch: 18 [16640/25569 (65%)]\tLoss: 0.192763\n",
      "Train Epoch: 18 [17280/25569 (68%)]\tLoss: 0.187193\n",
      "Train Epoch: 18 [17920/25569 (70%)]\tLoss: 0.246472\n",
      "Train Epoch: 18 [18560/25569 (72%)]\tLoss: 0.223334\n",
      "Train Epoch: 18 [19200/25569 (75%)]\tLoss: 0.158260\n",
      "Train Epoch: 18 [19840/25569 (78%)]\tLoss: 0.185175\n",
      "Train Epoch: 18 [20480/25569 (80%)]\tLoss: 0.270723\n",
      "Train Epoch: 18 [21120/25569 (82%)]\tLoss: 0.318493\n",
      "Train Epoch: 18 [21760/25569 (85%)]\tLoss: 0.106707\n",
      "Train Epoch: 18 [22400/25569 (88%)]\tLoss: 0.065098\n",
      "Train Epoch: 18 [23040/25569 (90%)]\tLoss: 0.146358\n",
      "Train Epoch: 18 [23680/25569 (92%)]\tLoss: 0.220359\n",
      "Train Epoch: 18 [24320/25569 (95%)]\tLoss: 0.115850\n",
      "Train Epoch: 18 [24960/25569 (98%)]\tLoss: 0.280912\n",
      "\n",
      "Test set: Average loss: 0.2046, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 19 [0/25569 (0%)]\tLoss: 0.123748\n",
      "Train Epoch: 19 [640/25569 (2%)]\tLoss: 0.265399\n",
      "Train Epoch: 19 [1280/25569 (5%)]\tLoss: 0.202874\n",
      "Train Epoch: 19 [1920/25569 (8%)]\tLoss: 0.109593\n",
      "Train Epoch: 19 [2560/25569 (10%)]\tLoss: 0.280192\n",
      "Train Epoch: 19 [3200/25569 (12%)]\tLoss: 0.229921\n",
      "Train Epoch: 19 [3840/25569 (15%)]\tLoss: 0.142104\n",
      "Train Epoch: 19 [4480/25569 (18%)]\tLoss: 0.256863\n",
      "Train Epoch: 19 [5120/25569 (20%)]\tLoss: 0.255335\n",
      "Train Epoch: 19 [5760/25569 (22%)]\tLoss: 0.246470\n",
      "Train Epoch: 19 [6400/25569 (25%)]\tLoss: 0.150465\n",
      "Train Epoch: 19 [7040/25569 (28%)]\tLoss: 0.188244\n",
      "Train Epoch: 19 [7680/25569 (30%)]\tLoss: 0.221300\n",
      "Train Epoch: 19 [8320/25569 (32%)]\tLoss: 0.192728\n",
      "Train Epoch: 19 [8960/25569 (35%)]\tLoss: 0.210438\n",
      "Train Epoch: 19 [9600/25569 (38%)]\tLoss: 0.102320\n",
      "Train Epoch: 19 [10240/25569 (40%)]\tLoss: 0.110816\n",
      "Train Epoch: 19 [10880/25569 (42%)]\tLoss: 0.105354\n",
      "Train Epoch: 19 [11520/25569 (45%)]\tLoss: 0.243822\n",
      "Train Epoch: 19 [12160/25569 (48%)]\tLoss: 0.182127\n",
      "Train Epoch: 19 [12800/25569 (50%)]\tLoss: 0.269347\n",
      "Train Epoch: 19 [13440/25569 (52%)]\tLoss: 0.185346\n",
      "Train Epoch: 19 [14080/25569 (55%)]\tLoss: 0.297708\n",
      "Train Epoch: 19 [14720/25569 (58%)]\tLoss: 0.365364\n",
      "Train Epoch: 19 [15360/25569 (60%)]\tLoss: 0.161797\n",
      "Train Epoch: 19 [16000/25569 (62%)]\tLoss: 0.243237\n",
      "Train Epoch: 19 [16640/25569 (65%)]\tLoss: 0.168997\n",
      "Train Epoch: 19 [17280/25569 (68%)]\tLoss: 0.264856\n",
      "Train Epoch: 19 [17920/25569 (70%)]\tLoss: 0.252744\n",
      "Train Epoch: 19 [18560/25569 (72%)]\tLoss: 0.107287\n",
      "Train Epoch: 19 [19200/25569 (75%)]\tLoss: 0.246202\n",
      "Train Epoch: 19 [19840/25569 (78%)]\tLoss: 0.160961\n",
      "Train Epoch: 19 [20480/25569 (80%)]\tLoss: 0.179354\n",
      "Train Epoch: 19 [21120/25569 (82%)]\tLoss: 0.145590\n",
      "Train Epoch: 19 [21760/25569 (85%)]\tLoss: 0.202910\n",
      "Train Epoch: 19 [22400/25569 (88%)]\tLoss: 0.200139\n",
      "Train Epoch: 19 [23040/25569 (90%)]\tLoss: 0.292402\n",
      "Train Epoch: 19 [23680/25569 (92%)]\tLoss: 0.184217\n",
      "Train Epoch: 19 [24320/25569 (95%)]\tLoss: 0.115601\n",
      "Train Epoch: 19 [24960/25569 (98%)]\tLoss: 0.130536\n",
      "\n",
      "Test set: Average loss: 0.2044, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "Train Epoch: 20 [0/25569 (0%)]\tLoss: 0.188143\n",
      "Train Epoch: 20 [640/25569 (2%)]\tLoss: 0.183728\n",
      "Train Epoch: 20 [1280/25569 (5%)]\tLoss: 0.381580\n",
      "Train Epoch: 20 [1920/25569 (8%)]\tLoss: 0.270912\n",
      "Train Epoch: 20 [2560/25569 (10%)]\tLoss: 0.247339\n",
      "Train Epoch: 20 [3200/25569 (12%)]\tLoss: 0.275789\n",
      "Train Epoch: 20 [3840/25569 (15%)]\tLoss: 0.321078\n",
      "Train Epoch: 20 [4480/25569 (18%)]\tLoss: 0.167475\n",
      "Train Epoch: 20 [5120/25569 (20%)]\tLoss: 0.108631\n",
      "Train Epoch: 20 [5760/25569 (22%)]\tLoss: 0.096803\n",
      "Train Epoch: 20 [6400/25569 (25%)]\tLoss: 0.150430\n",
      "Train Epoch: 20 [7040/25569 (28%)]\tLoss: 0.174912\n",
      "Train Epoch: 20 [7680/25569 (30%)]\tLoss: 0.263385\n",
      "Train Epoch: 20 [8320/25569 (32%)]\tLoss: 0.146073\n",
      "Train Epoch: 20 [8960/25569 (35%)]\tLoss: 0.177904\n",
      "Train Epoch: 20 [9600/25569 (38%)]\tLoss: 0.270711\n",
      "Train Epoch: 20 [10240/25569 (40%)]\tLoss: 0.249800\n",
      "Train Epoch: 20 [10880/25569 (42%)]\tLoss: 0.249697\n",
      "Train Epoch: 20 [11520/25569 (45%)]\tLoss: 0.195201\n",
      "Train Epoch: 20 [12160/25569 (48%)]\tLoss: 0.141478\n",
      "Train Epoch: 20 [12800/25569 (50%)]\tLoss: 0.119751\n",
      "Train Epoch: 20 [13440/25569 (52%)]\tLoss: 0.270711\n",
      "Train Epoch: 20 [14080/25569 (55%)]\tLoss: 0.110788\n",
      "Train Epoch: 20 [14720/25569 (58%)]\tLoss: 0.186846\n",
      "Train Epoch: 20 [15360/25569 (60%)]\tLoss: 0.192030\n",
      "Train Epoch: 20 [16000/25569 (62%)]\tLoss: 0.180900\n",
      "Train Epoch: 20 [16640/25569 (65%)]\tLoss: 0.116012\n",
      "Train Epoch: 20 [17280/25569 (68%)]\tLoss: 0.291902\n",
      "Train Epoch: 20 [17920/25569 (70%)]\tLoss: 0.295867\n",
      "Train Epoch: 20 [18560/25569 (72%)]\tLoss: 0.285293\n",
      "Train Epoch: 20 [19200/25569 (75%)]\tLoss: 0.225436\n",
      "Train Epoch: 20 [19840/25569 (78%)]\tLoss: 0.157873\n",
      "Train Epoch: 20 [20480/25569 (80%)]\tLoss: 0.084808\n",
      "Train Epoch: 20 [21120/25569 (82%)]\tLoss: 0.078084\n",
      "Train Epoch: 20 [21760/25569 (85%)]\tLoss: 0.228304\n",
      "Train Epoch: 20 [22400/25569 (88%)]\tLoss: 0.108967\n",
      "Train Epoch: 20 [23040/25569 (90%)]\tLoss: 0.110027\n",
      "Train Epoch: 20 [23680/25569 (92%)]\tLoss: 0.230374\n",
      "Train Epoch: 20 [24320/25569 (95%)]\tLoss: 0.331087\n",
      "Train Epoch: 20 [24960/25569 (98%)]\tLoss: 0.119634\n",
      "\n",
      "Test set: Average loss: 0.2044, Accuracy: 2967/3196 (93%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1995, Accuracy: 2970/3197 (93%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.89959336878323"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = RNNModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_rnn, y_train_rnn), batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_rnn, y_val_rnn), batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_rnn, y_test_rnn), batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    acc = test(args, model, device, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "# Test the model\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.99\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setup grid search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "y\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# the grid search aims to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(best_rf, 'model/rf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = load('model/rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_resampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_rf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_resampled\u001b[49m, y_train_resampled)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_resampled' is not defined"
     ]
    }
   ],
   "source": [
    "best_rf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9396309039724742\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      2970\n",
      "           1       0.70      0.26      0.38       227\n",
      "\n",
      "    accuracy                           0.94      3197\n",
      "   macro avg       0.82      0.63      0.68      3197\n",
      "weighted avg       0.93      0.94      0.93      3197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_rf.predict(X_test)\n",
    "RF_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", RF_accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction on text\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text)  \n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english'))  # Define stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "def tweet_to_single_vector(tweet, model):\n",
    "    word_vectors = [model.wv[word] for word in tweet if word in model.wv]\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0) \n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  \n",
    "\n",
    "def predict_rf(text):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    vector = tweet_to_single_vector(cleaned_text, model)\n",
    "    return best_rf.predict([vector])[0]\n",
    "\n",
    "predict_rf(\"I am happy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BayesianModel.__init__() takes from 1 to 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We first create a model which containts edges of the graph\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBayesianModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUsername\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet_vectors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRetweet Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMention Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFollower Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVerified\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCreated At\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHashtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBot Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: BayesianModel.__init__() takes from 1 to 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# We first create a model which containts edges of the graph\n",
    "model = BayesianModel([('Username', 'Bot Label'), ('tweet_vectors', 'Bot Label'), ('Retweet Count', 'Bot Label'), ('Mention Count', 'Bot Label'), ('Follower Count', 'Bot Label'), ('Verified', 'Bot Label')], ('Location', 'Bot Label'), ('Created At', 'Bot Label'),('Hashtags', 'Bot Label'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
