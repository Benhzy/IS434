{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Output Classifier with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "main_df = pd.read_csv('data/main.csv')  \n",
    "main_df = main_df.dropna()\n",
    "#drop rows with label = 1\n",
    "main_df = main_df[main_df.label != 1]\n",
    "main_df.head()\n",
    "\n",
    "save_path = 'data/main_no_1.csv'\n",
    "main_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Step 4: Prepare the Features\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Use TF-IDF Vectorization on 'tweet_text'\u001b[39;00m\n\u001b[0;32m     30\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)  \u001b[38;5;66;03m# You can adjust max_features\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Step 5: Train a Multi-Label Classification Model\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[0;32m     35\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     36\u001b[0m     X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1272\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     j_indices\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1271\u001b[0m     values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m-> 1272\u001b[0m     indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(j_indices))\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fixed_vocab:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;66;03m# disable defaultdict behaviour\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Merge the CSV Files\n",
    "# Load the CSV files (adjust file names and paths as necessary)\n",
    "tweets_df = pd.read_csv('data/tweet_labelled_cleaned_topics.csv')  # Contains 'tweet_text' and 'topic_number'\n",
    "tags_df = pd.read_csv('data/labels.csv')  \n",
    "\n",
    "# Merge on 'topic_number'\n",
    "merged_df = pd.merge(tweets_df, tags_df, on='tweet_topics', how='left')\n",
    "\n",
    "# add main_df to merged_df \n",
    "\n",
    "merged_df = pd.merge(merged_df, main_df, on='tweet', how='left')\n",
    "\n",
    "# Step 2: Prepare the Tags for One-Hot Encoding\n",
    "merged_df['tags'] = merged_df['tags'].apply(lambda x: x.lower().split(','))\n",
    "\n",
    "# Step 3: One-Hot Encode the Tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(merged_df['tags'])\n",
    "\n",
    "# Step 4: Prepare the Features\n",
    "# Use TF-IDF Vectorization on 'tweet_text'\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features\n",
    "X = vectorizer.fit_transform(merged_df['tweet'])\n",
    "\n",
    "# Step 5: Train a Multi-Label Classification Model\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Use MultiOutputClassifier with Logistic Regression\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "Y_pred = model.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred, target_names=mlb.classes_))\n",
    "\n",
    "# Step 7: Predict Tags for New Tweets\n",
    "new_tweets = ['This is a new tweet about topic X', 'Another tweet about topic Y']\n",
    "X_new = vectorizer.transform(new_tweets)\n",
    "Y_new_pred = model.predict(X_new)\n",
    "tags_predicted = mlb.inverse_transform(Y_new_pred)\n",
    "for tweet, tags in zip(new_tweets, tags_predicted):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted Tags: {tags}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "import joblib\n",
    "joblib.dump(model, 'model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: the government is doing a great job\n",
      "Predicted Tags: ('political_polarisation', 'vulgarity')\n",
      "\n",
      "Tweet: the government is doing a terrible job\n",
      "Predicted Tags: ('political_polarisation', 'vulgarity')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the model to predict if a tweet is about a topic\n",
    "model = joblib.load('model.pkl')\n",
    "new_tweets = ['the government is doing a great job', 'the government is doing a terrible job']\n",
    "X_new = vectorizer.transform(new_tweets)\n",
    "Y_new_pred = model.predict(X_new)\n",
    "tags_predicted = mlb.inverse_transform(Y_new_pred)\n",
    "for tweet, tags in zip(new_tweets, tags_predicted):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted Tags: {tags}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
