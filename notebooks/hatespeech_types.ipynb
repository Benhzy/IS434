{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS434 Twitter/X Bot Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Project aims to use machine learning and analytics techniques to detect bot accounts on the Twitter/X platform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgmpy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (0.1.26)\n",
      "Requirement already satisfied: networkx in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (2.2.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (3.1.4)\n",
      "Requirement already satisfied: torch in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (2.4.1)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (0.14.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (1.4.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (3.4.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (2.1.1)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pgmpy) (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.20.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.148.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.35.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (4.25.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-generativeai->pgmpy) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pandas->pgmpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pandas->pgmpy) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pandas->pgmpy) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn->pgmpy) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from statsmodels->pgmpy) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from statsmodels->pgmpy) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from torch->pgmpy) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tqdm->pgmpy) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core->google-generativeai->pgmpy) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core->google-generativeai->pgmpy) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (4.9)\n",
      "Requirement already satisfied: six in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from patsy>=0.5.6->statsmodels->pgmpy) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from jinja2->torch->pgmpy) (3.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pydantic->google-generativeai->pgmpy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pydantic->google-generativeai->pgmpy) (2.23.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from sympy->torch->pgmpy) (1.3.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.66.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.66.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->pgmpy) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2024.8.30)\n",
      "Collecting protobuf (from google-generativeai->pgmpy)\n",
      "  Using cached protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Using cached protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "Successfully installed protobuf-5.28.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\benhz\\documents\\github\\is434\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "Successfully installed protobuf-4.25.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.66.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pgmpy\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install imblearn\n",
    "!pip install keras\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>ableism</th>\n",
       "      <th>anti_religion</th>\n",
       "      <th>harm</th>\n",
       "      <th>homophobia</th>\n",
       "      <th>islamophobia</th>\n",
       "      <th>lookism</th>\n",
       "      <th>political_polarisation</th>\n",
       "      <th>racism</th>\n",
       "      <th>religious_intolerance</th>\n",
       "      <th>sexism</th>\n",
       "      <th>vulgarity</th>\n",
       "      <th>xenophobia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>`  :Click on my ``Annoying Users`` link! I gue...</td>\n",
       "      <td>['click', 'annoying', 'user', 'link', 'guess',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iraq is not good  ===  ===  USA is bad</td>\n",
       "      <td>['iraq', 'good', 'usa', 'bad']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>`  Buddha - ``Some suggest that victims should...</td>\n",
       "      <td>['buddha', 'suggest', 'victim', 'referred', 'a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>____ fuck off you little asshole. If you wan...</td>\n",
       "      <td>['____', 'fuck', 'little', 'asshole', 'want', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have a dick, its bigger than yours! hahaha</td>\n",
       "      <td>['dick', 'bigger', 'hahaha']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  `  :Click on my ``Annoying Users`` link! I gue...   \n",
       "1          Iraq is not good  ===  ===  USA is bad      \n",
       "2  `  Buddha - ``Some suggest that victims should...   \n",
       "3    ____ fuck off you little asshole. If you wan...   \n",
       "4       i have a dick, its bigger than yours! hahaha   \n",
       "\n",
       "                                     processed_tweet  ableism  anti_religion  \\\n",
       "0  ['click', 'annoying', 'user', 'link', 'guess',...        0              0   \n",
       "1                     ['iraq', 'good', 'usa', 'bad']        0              0   \n",
       "2  ['buddha', 'suggest', 'victim', 'referred', 'a...        0              0   \n",
       "3  ['____', 'fuck', 'little', 'asshole', 'want', ...        1              0   \n",
       "4                       ['dick', 'bigger', 'hahaha']        1              0   \n",
       "\n",
       "   harm  homophobia  islamophobia  lookism  political_polarisation  racism  \\\n",
       "0     0           0             0        0                       1       0   \n",
       "1     0           0             0        0                       1       0   \n",
       "2     0           0             0        0                       1       0   \n",
       "3     0           1             0        0                       0       1   \n",
       "4     0           1             0        0                       0       1   \n",
       "\n",
       "   religious_intolerance  sexism  vulgarity  xenophobia  \n",
       "0                      0       0          1           0  \n",
       "1                      0       0          1           0  \n",
       "2                      0       0          1           0  \n",
       "3                      0       0          1           1  \n",
       "4                      0       0          1           1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Merge the CSV Files\n",
    "# Load the CSV files (adjust file names and paths as necessary)\n",
    "tweets_df = pd.read_csv('data/tweet_labelled_cleaned_topics.csv')  # Contains 'tweet_text' and 'topic_number'\n",
    "tags_df = pd.read_csv('data/labels.csv')      \n",
    "\n",
    "# Merge on 'topic_number'\n",
    "merged_df = pd.merge(tweets_df, tags_df, on='tweet_topics', how='left')\n",
    "\n",
    "# Step 2: Prepare the Tags for One-Hot Encoding\n",
    "merged_df['tags'] = merged_df['tags'].apply(lambda x: x.lower().split(','))\n",
    "\n",
    "# Step 3: One-Hot Encode the Tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "tags_encoded = mlb.fit_transform(merged_df['tags'])\n",
    "tags_encoded_df = pd.DataFrame(tags_encoded, columns=mlb.classes_)\n",
    "merged_df = pd.concat([merged_df, tags_encoded_df], axis=1)\n",
    "merged_df.drop(columns=['tags','index','tweet_topics','label'], inplace=True)\n",
    "\n",
    "save_path = 'data/merged_df.csv'\n",
    "merged_df.to_csv(save_path, index=False)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.read_csv('data/merged_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Average Training Loss: 0.6173\n",
      "Validation Loss: 0.4162\n",
      "Validation Macro F1: 0.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Average Training Loss: 0.3715\n",
      "Validation Loss: 0.3466\n",
      "Validation Macro F1: 0.8071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Average Training Loss: 0.2644\n",
      "Validation Loss: 0.3805\n",
      "Validation Macro F1: 0.8568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Average Training Loss: 0.2005\n",
      "Validation Loss: 0.4428\n",
      "Validation Macro F1: 0.8655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Average Training Loss: 0.1686\n",
      "Validation Loss: 0.5450\n",
      "Validation Macro F1: 0.8897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Average Training Loss: 0.0862\n",
      "Validation Loss: 0.4987\n",
      "Validation Macro F1: 0.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Average Training Loss: 0.0633\n",
      "Validation Loss: 0.5313\n",
      "Validation Macro F1: 0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Average Training Loss: 0.0528\n",
      "Validation Loss: 0.5550\n",
      "Validation Macro F1: 0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Average Training Loss: 0.0407\n",
      "Validation Loss: 0.5654\n",
      "Validation Macro F1: 0.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Average Training Loss: 0.0390\n",
      "Validation Loss: 0.5873\n",
      "Validation Macro F1: 0.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\benhz\\Documents\\GitHub\\IS434\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Performance:\n",
      "Test Loss: 0.5583\n",
      "\n",
      "Detailed Metrics:\n",
      "\n",
      "ableism:\n",
      "Precision: 0.8984\n",
      "Recall: 0.9074\n",
      "F1-score: 0.9029\n",
      "\n",
      "anti_religion:\n",
      "Precision: 0.8489\n",
      "Recall: 0.8741\n",
      "F1-score: 0.8613\n",
      "\n",
      "harm:\n",
      "Precision: 0.9027\n",
      "Recall: 0.8855\n",
      "F1-score: 0.8940\n",
      "\n",
      "homophobia:\n",
      "Precision: 0.8835\n",
      "Recall: 0.8955\n",
      "F1-score: 0.8895\n",
      "\n",
      "islamophobia:\n",
      "Precision: 0.8225\n",
      "Recall: 0.8203\n",
      "F1-score: 0.8214\n",
      "\n",
      "lookism:\n",
      "Precision: 0.9027\n",
      "Recall: 0.8855\n",
      "F1-score: 0.8940\n",
      "\n",
      "political_polarisation:\n",
      "Precision: 0.9109\n",
      "Recall: 0.9224\n",
      "F1-score: 0.9166\n",
      "\n",
      "racism:\n",
      "Precision: 0.9246\n",
      "Recall: 0.9202\n",
      "F1-score: 0.9224\n",
      "\n",
      "religious_intolerance:\n",
      "Precision: 0.8956\n",
      "Recall: 0.9047\n",
      "F1-score: 0.9001\n",
      "\n",
      "sexism:\n",
      "Precision: 0.9161\n",
      "Recall: 0.9024\n",
      "F1-score: 0.9092\n",
      "\n",
      "vulgarity:\n",
      "Precision: 0.9499\n",
      "Recall: 0.9583\n",
      "F1-score: 0.9541\n",
      "\n",
      "xenophobia:\n",
      "Precision: 0.9052\n",
      "Recall: 0.9063\n",
      "F1-score: 0.9058\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import Counter\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Label columns (using your existing ones)\n",
    "label_columns = [\n",
    "    \"ableism\", \"anti_religion\", \"harm\", \"homophobia\", \"islamophobia\", \"lookism\",\n",
    "    \"political_polarisation\", \"racism\", \"religious_intolerance\", \"sexism\", \"vulgarity\", \"xenophobia\"\n",
    "]\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Clean text\n",
    "        text = self._clean_text(text)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        # Basic text cleaning\n",
    "        text = text.lower().strip()\n",
    "        # Remove URLs\n",
    "        text = ' '.join(word for word in text.split() \n",
    "                       if not word.startswith('http') and not word.startswith('www'))\n",
    "        return text\n",
    "\n",
    "class LSTMHateSpeech(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(LSTMHateSpeech, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Multiple dense layers with batch normalization\n",
    "        self.dense1 = nn.Linear(768, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.classifier = nn.Linear(256, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        \n",
    "        x = torch.relu(self.bn1(self.dense1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.dense2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "# Prepare data\n",
    "texts = merged_df['tweet'].values\n",
    "labels = merged_df[label_columns].values\n",
    "\n",
    "# Calculate dataset sizes\n",
    "dataset_size = len(texts)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = TweetDataset(texts, labels, tokenizer)\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "def calculate_class_weights(labels):\n",
    "    pos_weights = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        neg_count = len(labels[labels[:, i] == 0])\n",
    "        pos_count = len(labels[labels[:, i] == 1])\n",
    "        pos_weights.append((neg_count + 1) / (pos_count + 1))\n",
    "    return torch.tensor(pos_weights, dtype=torch.float).to(device)\n",
    "\n",
    "class_weights = calculate_class_weights(labels)\n",
    "\n",
    "# Initialize model and training components\n",
    "model = LSTMHateSpeech(len(label_columns)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(outputs) >= 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    metrics = classification_report(\n",
    "        np.array(all_labels), \n",
    "        np.array(all_preds), \n",
    "        target_names=label_columns,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_f1 = 0\n",
    "best_model = None\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_metrics = evaluate_model(model, val_loader, criterion)\n",
    "    val_f1 = val_metrics['macro avg']['f1-score']\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping with model saving\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_loss, test_metrics = evaluate_model(model, test_loader, criterion)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "for label in label_columns:\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"Precision: {test_metrics[label]['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics[label]['recall']:.4f}\")\n",
    "    print(f\"F1-score: {test_metrics[label]['f1-score']:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'hate_speech_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as bilstm_attention_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model/hate_speech_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benhz\\AppData\\Local\\Temp\\ipykernel_27624\\3778486265.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('hate_speech_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImprovedHateSpeechModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (dense1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dense2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Linear(in_features=256, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = LSTMHateSpeech(len(label_columns)).to(device)\n",
    "model.load_state_dict(torch.load('hate_speech_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet, model, tokenizer, threshold=0.7):\n",
    "    model.eval()\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    token_type_ids = encoded['token_type_ids'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "    \n",
    "    preds = torch.sigmoid(outputs) >= threshold\n",
    "    pred_labels = [label_columns[i] for i, pred in enumerate(preds.squeeze()) if pred == 1]\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: All Chinese people are disgusting\n",
      "All Chinese people are disgusting\n",
      "Predicted Labels: ['ableism', 'homophobia', 'racism', 'vulgarity', 'xenophobia']\n",
      "\n",
      "Tweet: Exams are coming, and I am not ready\n",
      "Exams are coming, and I am not ready\n",
      "Predicted Labels: []\n",
      "\n",
      "Tweet: Children are so annoying\n",
      "Children are so annoying\n",
      "Predicted Labels: ['political_polarisation', 'vulgarity']\n",
      "\n",
      "Tweet: I hate my job\n",
      "I hate my job\n",
      "Predicted Labels: ['islamophobia', 'political_polarisation', 'racism', 'xenophobia']\n",
      "\n",
      "Tweet: Chinese should go back to their country\n",
      "Chinese should go back to their country\n",
      "Predicted Labels: ['ableism', 'homophobia', 'racism', 'vulgarity', 'xenophobia']\n",
      "\n",
      "Tweet: Asians should not be allowed to drive\n",
      "Asians should not be allowed to drive\n",
      "Predicted Labels: ['political_polarisation', 'vulgarity']\n",
      "\n",
      "Tweet: The storm last night was crazy\n",
      "The storm last night was crazy\n",
      "Predicted Labels: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = [\n",
    "    \"All Chinese people are disgusting\",\n",
    "    \"Exams are coming, and I am not ready\",\n",
    "    \"Children are so annoying\",\n",
    "    \"I hate my job\",\n",
    "    \"Chinese should go back to their country\",\n",
    "    \"Asians should not be allowed to drive\",\n",
    "    \"The storm last night was crazy\"\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    predictions = predict(tweet, model, tokenizer)\n",
    "    print(tweet)\n",
    "    print(f\"Predicted Labels: {predictions}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
